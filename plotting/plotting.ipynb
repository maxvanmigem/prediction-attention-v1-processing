{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mne, os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from mne.time_frequency import tfr_morlet\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg  width=\"550\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#1f77b4;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#ff7f0e;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"110\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#2ca02c;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"165\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#d62728;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"220\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#9467bd;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"275\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#8c564b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"330\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#e377c2;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"385\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#7f7f7f;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"440\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#bcbd22;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"495\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#17becf;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ],
      "text/plain": [
       "['#1f77b4',\n",
       " '#ff7f0e',\n",
       " '#2ca02c',\n",
       " '#d62728',\n",
       " '#9467bd',\n",
       " '#8c564b',\n",
       " '#e377c2',\n",
       " '#7f7f7f',\n",
       " '#bcbd22',\n",
       " '#17becf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "***Aestetics***\n",
    "\"\"\"\n",
    "# load color palette\n",
    "my_palette = sns.color_palette().as_hex()\n",
    "sns.color_palette().as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_01-epo.fif ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4189 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_02-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4196 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_04-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_05-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4054 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_06-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_07-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "3918 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_08-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "3506 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_09-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "3425 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_10-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_11-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4195 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_12-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4194 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_13-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_15-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4199 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_16-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_17-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_18-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4195 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_19-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4197 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_21-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_22-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4198 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_23-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4198 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_24-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_25-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4186 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_26-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4197 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvmigem\\AppData\\Local\\Temp\\ipykernel_16672\\1364701117.py:40: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  eps = mne.concatenate_epochs(ep_list).crop(tmin=epoch_tmin,tmax=epoch_tmax).apply_baseline()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 35 columns\n",
      "94647 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Load data\n",
    "\"\"\"\n",
    "# Big list\n",
    "ep_list = []\n",
    "# Sup grouped list\n",
    "ep_group1 = []\n",
    "ep_group2 = []\n",
    "ep_group3 = []\n",
    "ep_group4 = []\n",
    "subjects = []\n",
    "# Specify the epoch length we will be looking at\n",
    "epoch_tmin = -0.1\n",
    "epoch_tmax = 0.5\n",
    "\n",
    "cleaned_data_dir = '/Users/mvmigem/Documents/data/project_1/preprocessed/mastoid_ref/'\n",
    "dir_list = glob.glob(cleaned_data_dir+'*-epo.fif')\n",
    "excuded_pp = [3,14,20]\n",
    "\n",
    "for i,sub_path in enumerate(dir_list):\n",
    "    sub = int(sub_path.split('main_eventset_mastoidref_')[1].split('-epo.fif')[0])\n",
    "    if sub in excuded_pp:\n",
    "        continue\n",
    "    clean_epoch_path = sub_path\n",
    "    epochs = mne.read_epochs(clean_epoch_path)\n",
    "    epochs.info['bads']= []\n",
    "    ep_list.append(epochs)\n",
    "\n",
    "    # if epochs.metadata['loc_quad'].iloc[0] == 0:\n",
    "    #     ep_group1.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 1:\n",
    "    #     ep_group2.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 2:\n",
    "    #     ep_group3.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 3:\n",
    "    #     ep_group4.append(epochs)\n",
    "    print(epochs.info['nchan'])\n",
    "\n",
    "# Concat list into big epoch object\n",
    "eps = mne.concatenate_epochs(ep_list).crop(tmin=epoch_tmin,tmax=epoch_tmax).apply_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_list = eps.metadata['participant'].unique() \n",
    "\n",
    "# Divide the epoch file into sections based on metadata that can't be distinguished by event names\n",
    "# Drop catch trials\n",
    "epochs_nocatch = eps.copy()['catch_trial == 0']\n",
    "# # Divide attention conditions\n",
    "# epochs_attended = epochs_nocatch['attention == \"attended\"']\n",
    "# epochs_unattended = epochs_nocatch['attention == \"unattended\"']\n",
    "# # Divide by staring position\n",
    "# epochs_start3 = epochs_nocatch['start_position == 2']\n",
    "# epochs_start1 = epochs_nocatch['start_position == 0']\n",
    "# epochs_start2 = epochs_nocatch['start_position == 1']\n",
    "# epochs_start4 = epochs_nocatch['start_position == 3']\n",
    "# # Divide unattended trials by start pos\n",
    "# epochs_unattended_start3 = epochs_unattended['start_position == 2']\n",
    "# epochs_unattended_start1 = epochs_unattended['start_position == 0']\n",
    "# epochs_unattended_start2 = epochs_unattended['start_position == 1']\n",
    "# epochs_unattended_start4 = epochs_unattended['start_position == 3']\n",
    "# # Divide attented trials by start pos\n",
    "# epochs_attended_start3 = epochs_attended['start_position == 2']\n",
    "# epochs_attended_start1 = epochs_attended['start_position == 0']\n",
    "# epochs_attended_start2 = epochs_attended['start_position == 1']\n",
    "# epochs_attended_start4 = epochs_attended['start_position == 3']\n",
    "# # Extra devision for the odd boys\n",
    "# epochs_attended_reg = epochs_attended['expected ==\"regular\"']\n",
    "# epochs_attended_odd = epochs_attended['expected == \"odd\"']\n",
    "# epochs_unattended_reg = epochs_unattended['expected ==\"regular\"']\n",
    "# epochs_unattended_odd = epochs_unattended['expected == \"odd\"']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evoked plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main pos individual channels\n",
    "\"\"\"\n",
    "evoked_pos1_list = []\n",
    "evoked_pos2_list = []\n",
    "evoked_pos3_list = []\n",
    "evoked_pos4_list = []\n",
    "for i,sub in enumerate(sub_list):\n",
    "    evoked_pos1_list.append(epochs_nocatch['pos1'][f'participant == {sub}'].average())\n",
    "    evoked_pos2_list.append(epochs_nocatch['pos2'][f'participant == {sub}'].average())\n",
    "    evoked_pos3_list.append(epochs_nocatch['pos3'][f'participant == {sub}'].average())\n",
    "    evoked_pos4_list.append(epochs_nocatch['pos4'][f'participant == {sub}'].average())\n",
    "\n",
    "evoked_pos1 = mne.grand_average(evoked_pos1_list)\n",
    "evoked_pos2 = mne.grand_average(evoked_pos2_list)\n",
    "evoked_pos3 = mne.grand_average(evoked_pos3_list)\n",
    "evoked_pos4 = mne.grand_average(evoked_pos4_list)\n",
    "\n",
    "evokeds_list = [evoked_pos1,evoked_pos2,evoked_pos3,evoked_pos4]\n",
    "conds = ('pos1','pos2','pos3','pos4')\n",
    "# conds = ('seq1','seq2','seq3','seq4')\n",
    "\n",
    "norm = dict(zip(conds, evokeds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peak properties of localiser data\n",
    "peak_properties = pd.read_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_peak_properties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous peak_properties dataframe uses peak latencies \n",
    "# based on the localiser data which may not be optimal\n",
    "# so I'm now making another dataframe to select the latencies\n",
    "# of the main experiment\n",
    "\n",
    "evokeds_lists = [evoked_pos1_list,evoked_pos2_list,evoked_pos3_list,evoked_pos4_list]\n",
    "\n",
    "all_pos = []\n",
    "pos1 = []\n",
    "pos2 = []\n",
    "pos3 = []\n",
    "pos4 = []\n",
    "\n",
    "positions = [pos1,pos2,pos3,pos4]\n",
    "peak_variables = ['peak_channel','peak_latency','peak_amplitude','subject']\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub]\n",
    "    av_list = []\n",
    "    for ind,evoked in enumerate(evokeds_lists):\n",
    "        channel = the_row[f'pos{ind+1}_peak_channel'].iloc[0]\n",
    "        roi_ev = evoked[i].copy().pick(channel)\n",
    "        \n",
    "        # mode = 'pos'\n",
    "        # if ind == 0 or ind == 1:\n",
    "        #     mode = 'neg\n",
    "        ch,lat,amp = roi_ev.get_peak(ch_type='eeg',\n",
    "                                    tmin=0.06,tmax=0.09,\n",
    "                                    mode= 'abs',\n",
    "                                    return_amplitude=True)\n",
    "        positions[ind].append(dict(zip(peak_variables,(ch,lat,amp,sub))))\n",
    "\n",
    "        # rms the tailored latencies\n",
    "        ev_data = evoked[i].data\n",
    "        rootsqr_data = np.sqrt((ev_data**2))\n",
    "        rsq_ev = mne.EvokedArray(rootsqr_data,evoked[i].info,tmin=evoked[i].times[0])\n",
    "        av_list.append(rsq_ev)\n",
    "    # also for the tailored latencies\n",
    "    all_pos_channel = the_row['all_pos_peak_channel'].iloc[0]\n",
    "    merged_ev = mne.grand_average(av_list)\n",
    "    ch,lat,amp = merged_ev.get_peak(ch_type='eeg',   \n",
    "                                tmin=0.06,tmax=0.09,\n",
    "                                return_amplitude=True)\n",
    "    all_pos.append(dict(zip(peak_variables,(ch,lat,amp))))\n",
    "\n",
    "main_all_pos = pd.DataFrame(all_pos)\n",
    "main_all_pos.rename(columns={'peak_channel':'all_pos_peak_channel',\n",
    "                             'peak_latency':'all_pos_peak_latency',\n",
    "                             'peak_amplitude':'all_pos_peak_amplitude'},inplace=True)\n",
    "main_pos1 = pd.DataFrame(positions[0])\n",
    "main_pos1.rename(columns={'peak_channel':'pos1_peak_channel',\n",
    "                          'peak_latency':'pos1_peak_latency',\n",
    "                          'peak_amplitude':'pos1_peak_amplitude'},inplace=True)\n",
    "main_pos1.drop(columns=['subject'],inplace=True)\n",
    "main_pos2 = pd.DataFrame(positions[1])\n",
    "main_pos2.rename(columns={'peak_channel':'pos2_peak_channel',\n",
    "                          'peak_latency':'pos2_peak_latency',\n",
    "                          'peak_amplitude':'pos2_peak_amplitude'},inplace=True)\n",
    "main_pos2.drop(columns='subject',inplace=True)\n",
    "main_pos3 = pd.DataFrame(positions[2])\n",
    "main_pos3.rename(columns={'peak_channel':'pos3_peak_channel',\n",
    "                          'peak_latency':'pos3_peak_latency',\n",
    "                          'peak_amplitude':'pos3_peak_amplitude'},inplace=True)\n",
    "main_pos3.drop(columns='subject',inplace=True)\n",
    "main_pos4 = pd.DataFrame(positions[3])\n",
    "main_pos4.rename(columns={'peak_channel':'pos4_peak_channel',\n",
    "                          'peak_latency':'pos4_peak_latency',\n",
    "                          'peak_amplitude':'pos4_peak_amplitude'},inplace=True)\n",
    "main_pos4.drop(columns='subject',inplace=True)\n",
    "general_pos = peak_properties[peak_properties['subject'].isin(sub_list)]\n",
    "\n",
    "general_pos = general_pos[['grand_average_peak_channel',\n",
    "                                'grand_average_peak_latency',\n",
    "                                'grand_average_peak_amplitude',\n",
    "                                'subject']].reset_index(drop=True)\n",
    "\n",
    "main_pos = [main_all_pos,main_pos1,main_pos2,main_pos3,main_pos4,general_pos]\n",
    "main_pos_df = pd.concat(main_pos,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_properties = main_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sub in enumerate(sub_list):\n",
    "    \n",
    "    evokeds_list = [evoked_pos1_list[i],evoked_pos2_list[i],evoked_pos3_list[i],evoked_pos4_list[i]]\n",
    "    conds = ('pos1','pos2','pos3','pos4')\n",
    "    # conds = ('seq1','seq2','seq3','seq4')\n",
    "    norm = dict(zip(conds, evokeds_list))\n",
    "    # mne.viz.plot_compare_evokeds(norm,picks=['POz'],vlines=[0.05,0.1],ylim=dict(eeg=[-6,6]))\n",
    "    epochs_nocatch['pos1'][f'participant == {sub}'].plot_image(picks=['POz','Pz','Oz'],vmin=-80,vmax=80,combine='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [-12,12]\n",
    "\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub]\n",
    "    channel = the_row['all_pos_peak_channel'].iloc[0]\n",
    "\n",
    "    ev_names = ('pos1','pos2','pos3','pos4')\n",
    "    evokeds_list = [evoked_pos1_list[i],evoked_pos2_list[i],evoked_pos3_list[i],evoked_pos4_list[i]]\n",
    "    # conds = ('seq1','seq2','seq3','seq4')\n",
    "    norm = dict(zip(ev_names, evokeds_list))\n",
    "\n",
    "    mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[0], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    mne.viz.plot_compare_evokeds(norm, picks=channel, axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [-12,12]\n",
    "evokeds_lists = [evoked_pos1_list,evoked_pos2_list,evoked_pos3_list,evoked_pos4_list]\n",
    "for i,sub in enumerate(sub_list):\n",
    "    # init fig\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    # take times from single evoked ( should be all the same)\n",
    "    times = evoked_pos1_list[0].times\n",
    "    # take sub loc peak info\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub]\n",
    "    # iter over positions\n",
    "    for idx in range(4):\n",
    "        standard = evokeds_lists[idx][i].copy().pick('POz').data.squeeze() * 1e6\n",
    "        sns.lineplot(x=times, y=standard, ax = axes[0], label = f'Pos{idx+1} POz')\n",
    "        channel = the_row[f'pos{idx+1}_peak_channel'].iloc[0]\n",
    "        selected = evokeds_lists[idx][i].copy().pick(channel).data.squeeze() * 1e6\n",
    "        sns.lineplot(x=times, y=selected, ax = axes[1],label = f'Pos{idx+1} {channel}')\n",
    "\n",
    "    # mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "    axes[0].axhline(y=0, lw=1, c='black' )\n",
    "    axes[1].axhline(y=0, lw=1, c='black' )\n",
    "    axes[0].axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    axes[0].axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "    axes[0].set_title('Fixed channel')\n",
    "    axes[1].set_title('Variable channel')\n",
    "    axes[0].set_ylabel('µV')\n",
    "    axes[0].set_xlabel('time (s)')\n",
    "    axes[1].set_xlabel('time (s)')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event selection for prediction effects\n",
    "\n",
    "# Remove catch\n",
    "# epochs_nocatch = eps.copy()['catch_trial == 0']['pos4']\n",
    "# Select second stim of trial\n",
    "epochs_stim2 = epochs_nocatch['seq2']\n",
    "# Regular vs odd\n",
    "epochs_odd = epochs_stim2['expected == \"odd\"']\n",
    "epochs_reg = epochs_stim2['expected == \"regular\"']['precedes_odd == 1']\n",
    "# Attention\n",
    "ep_odd_att = epochs_odd['attention == \"attended\"']\n",
    "ep_odd_unatt = epochs_odd['attention == \"unattended\"']\n",
    "ep_reg_att = epochs_reg['attention == \"attended\"']\n",
    "ep_reg_unatt = epochs_reg['attention == \"unattended\"']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "checking prediction manipulation on p3, means agragating across positions \n",
    "\"\"\"\n",
    "att_reg_ev = []\n",
    "att_odd_ev = []\n",
    "unatt_reg_ev = []\n",
    "unatt_odd_ev = []\n",
    "# totals_grand = []\n",
    "\n",
    "\n",
    "for i, sub in enumerate(sub_list):\n",
    "    \n",
    "    att_reg_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'attended'\"]['expected == \"regular\"']['precedes_odd == 1'].average())\n",
    "    att_odd_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'attended'\"]['expected == \"odd\"'].average())\n",
    "    unatt_reg_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'unattended'\"]['expected == \"regular\"']['precedes_odd == 1'].average())\n",
    "    unatt_odd_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'unattended'\"]['expected == \"odd\"'].average())\n",
    "    \n",
    "    # totals_grand.append(\n",
    "    #     epochs_nocatch['seq2'].average())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evoked_pred = [mne.grand_average(att_reg_ev),\n",
    "               mne.grand_average(att_odd_ev),\n",
    "               mne.grand_average(unatt_reg_ev),\n",
    "               mne.grand_average(unatt_odd_ev),]\n",
    "\n",
    "conditions = ('attended regular','attended odd','unattended regular','unattended odd')\n",
    "\n",
    "norm = dict(zip(conditions, evoked_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "create df with the neededed values for p3 analysis\n",
    "\"\"\"\n",
    "# init df\n",
    "long_sub_list = [item for item in sub_list for _ in range(4)]\n",
    "p3_df = pd.DataFrame(long_sub_list, columns=['subject'])\n",
    "\n",
    "# selected channels\n",
    "roi = ['P1','Pz','P2','CP1','CPz','CP2','C1','Cz','C2']\n",
    "roi_ix = mne.pick_channels(epochs_nocatch[0].info[\"ch_names\"], include=roi)\n",
    "roi_dict = dict(centropar = list(roi_ix))\n",
    "p3_tmin, p3_tmax = 0.25, 0.45\n",
    "\n",
    "# loop subjects\n",
    "mid_att = []\n",
    "mid_pred = []\n",
    "mid_peak = []\n",
    "mid_lat = []\n",
    "mid_mean = []\n",
    "sub_check = []\n",
    "for i, sub in enumerate(sub_list):\n",
    "    print(sub)\n",
    "    # attended and regular\n",
    "    ev_roi = mne.channels.combine_channels(inst = att_reg_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('attended')\n",
    "    mid_pred.append('regular')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "    # attended and odd\n",
    "    ev_roi = mne.channels.combine_channels(inst = att_odd_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('attended')\n",
    "    mid_pred.append('odd')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "    # unattended and regular\n",
    "    ev_roi = mne.channels.combine_channels(inst = unatt_reg_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('unattended')\n",
    "    mid_pred.append('regular')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "    # unattended and odd\n",
    "    ev_roi = mne.channels.combine_channels(inst = unatt_odd_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('unattended')\n",
    "    mid_pred.append('odd')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "\n",
    "p3_df['attention'] = mid_att\n",
    "p3_df['expectation'] = mid_pred\n",
    "p3_df['mean_amp'] = mid_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\p3_df.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main effects \n",
    "\"\"\"\n",
    "reg_att_up = []\n",
    "odd_att_up = []\n",
    "reg_unatt_up = []\n",
    "odd_unatt_up = []\n",
    "\n",
    "reg_att_down = []\n",
    "odd_att_down = []\n",
    "reg_unatt_down = []\n",
    "odd_unatt_down = []\n",
    "\n",
    "cond_ep_lists = [ep_reg_att,ep_odd_att,ep_reg_unatt,ep_odd_unatt] \n",
    "up_conds = [reg_att_up,odd_att_up,reg_unatt_up,odd_unatt_up]  \n",
    "down_conds = [reg_att_down,odd_att_down,reg_unatt_down,odd_unatt_down]  \n",
    "ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "# List to know which position was used to select the up or down later\n",
    "sub_pos_info = []\n",
    "\n",
    "evokeds_up = dict(zip(ev_names,up_conds))\n",
    "evokeds_down = dict(zip(ev_names,down_conds))\n",
    "\n",
    "for i, sub in enumerate(sub_list):\n",
    "    # which set of positions does this sub have\n",
    "    loc_quad_values = ep_reg_att[f'participant == {sub}'].metadata[\"loc_quad\"]\n",
    "    loc_quad_value = loc_quad_values.unique()[0]\n",
    "    # Dict to stor positional info\n",
    "    sub_info_dict = {}\n",
    "    sub_info_dict['subject'] = sub\n",
    "    # Positions 1 and 3\n",
    "    if loc_quad_value == 0  or loc_quad_value== 2 :\n",
    "        # make dict to store the selected evokeds\n",
    "        ev_position1 = {}\n",
    "        # for every condition\n",
    "        for i, epoch in enumerate(cond_ep_lists):\n",
    "            # transform the epochs into evokeds\n",
    "            ev_position1[ev_names[i]] = epoch['pos1'][f'participant == {sub}'].average()\n",
    "        # same for position 3\n",
    "        ev_position3 = {}\n",
    "        for i, epoch in enumerate(cond_ep_lists):\n",
    "            ev_position3[ev_names[i]] = epoch['pos3'][f'participant == {sub}'].average()\n",
    "        # for every condition in the upper vf\n",
    "        for cond,evoked in ev_position1.items():\n",
    "            evokeds_up[cond].append(evoked)\n",
    "        # same for down\n",
    "        for cond,evoked in ev_position3.items():\n",
    "            evokeds_down[cond].append(evoked)\n",
    "        # Store position information\n",
    "        sub_info_dict['up_pos'] = 1\n",
    "        sub_info_dict['down_pos'] = 3\n",
    "    # same for positions 2 and 4\n",
    "    if loc_quad_value == 1  or loc_quad_value== 3 :\n",
    "        ev_position2 = {}\n",
    "        for i, evoked in enumerate(cond_ep_lists):\n",
    "            ev_position2[ev_names[i]] = evoked['pos2'][f'participant == {sub}'].average()\n",
    "        ev_position4 = {}\n",
    "        for i, evoked in enumerate(cond_ep_lists):\n",
    "            ev_position4[ev_names[i]] = evoked['pos4'][f'participant == {sub}'].average()\n",
    "        # for every condition in the upper vf\n",
    "        for cond,evoked in ev_position2.items():\n",
    "            # append to that conditions list in the up_dict\n",
    "            evokeds_up[cond].append(evoked)\n",
    "        # same for lower vf\n",
    "        for cond,evoked in ev_position4.items():\n",
    "            evokeds_down[cond].append(evoked)\n",
    "        sub_info_dict['up_pos'] = 2\n",
    "        sub_info_dict['down_pos'] = 4\n",
    "    # Add info_dict_to list\n",
    "    sub_pos_info.append(sub_info_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peak properties of localiser data\n",
    "peak_properties = pd.read_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_peak_properties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom dict of lists to list of dicts for plotting individually\n",
    "subj_uplist = [dict(zip(evokeds_up, values)) for values in zip(*evokeds_up.values())]\n",
    "subj_downlist = [dict(zip(evokeds_down, values)) for values in zip(*evokeds_down.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes for export and analysis\n",
    "long_sub_list = [item for item in sub_list for _ in range(4)]\n",
    "c1_long_df = pd.DataFrame(long_sub_list, columns=['subject'])\n",
    "window_halfwidth = 0.015\n",
    "# Condition columns\n",
    "mid_att = []\n",
    "mid_pred = []\n",
    "# General analysis list init \n",
    "genup_amp = []\n",
    "gendown_amp = []\n",
    "general_amp = []\n",
    "# General time window estimate\n",
    "gen_lat = peak_properties['grand_average_peak_latency'][0]\n",
    "gen_tmin = gen_lat - window_halfwidth\n",
    "gen_tmax = gen_lat + window_halfwidth\n",
    "# \n",
    "selectup_gen_evoked = []\n",
    "selectdown_gen_evoked = []\n",
    "\n",
    "# Tailored analysis empty lists\n",
    "tailup_amp = []\n",
    "taildown_amp = []\n",
    "tailored_amp =[]\n",
    "\n",
    "selectup_tail_evoked = []\n",
    "selectdown_tail_evoked = []\n",
    "\n",
    "# Ultra tailored analysis values \n",
    "ultraup_amp = []\n",
    "ultradown_amp = []\n",
    "ultra_amp = []\n",
    "ultraup_tmins = []\n",
    "ultraup_tmaxs = []\n",
    "ultradown_tmins = []\n",
    "ultradown_tmaxs = []\n",
    "\n",
    "selectup_ultra_evoked = []\n",
    "selectdown_ultra_evoked = []\n",
    "\n",
    "conditions_att = ['attended','attended','unattended','unattended',]\n",
    "conditions_pred = ['regular','odd','regular','odd',]\n",
    "conditions = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "\n",
    "# Static C1 window selection\n",
    "for i, sub in enumerate(sub_list):\n",
    "    # Dicts to store the selected evoked\n",
    "    upgen_select_dict = {}\n",
    "    downgen_select_dict = {}\n",
    "    uptail_select_dict = {}\n",
    "    downtail_select_dict = {}\n",
    "    upultra_select_dict = {}\n",
    "    downultra_select_dict = {}\n",
    "    # Select the channel and latency based on localiser info\n",
    "    sub_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    # Tailored info\n",
    "    tailored_channel = sub_row['all_pos_peak_channel'].iloc[0]\n",
    "    tailored_latency = sub_row['all_pos_peak_latency'].iloc[0]\n",
    "    tail_tmin = tailored_latency - window_halfwidth\n",
    "    tail_tmax = tailored_latency + window_halfwidth\n",
    "    # Ultra tailored info\n",
    "    up_pos = sub_pos_info[i]['up_pos']\n",
    "    down_pos = sub_pos_info[i]['down_pos']\n",
    "    ultraup_channel = sub_row[f'pos{up_pos}_peak_channel'].iloc[0]\n",
    "    ultradown_channel = sub_row[f'pos{down_pos}_peak_channel'].iloc[0]\n",
    "    ultraup_latency = sub_row[f'pos{up_pos}_peak_latency'].iloc[0]\n",
    "    ultradown_latency = sub_row[f'pos{up_pos}_peak_latency'].iloc[0]\n",
    "    ultraup_tmin = ultraup_latency - window_halfwidth\n",
    "    [ultraup_tmins.append(ultraup_tmin) for p in range(4)]\n",
    "    ultraup_tmax = ultraup_latency + window_halfwidth\n",
    "    [ultraup_tmaxs.append(ultraup_tmax) for p in range(4)]\n",
    "    ultradown_tmin = ultradown_latency - window_halfwidth\n",
    "    [ultradown_tmins.append(ultradown_tmin) for p in range(4)]\n",
    "    ultradown_tmax = ultradown_latency + window_halfwidth\n",
    "    [ultradown_tmaxs.append(ultradown_tmax) for p in range(4)]\n",
    "\n",
    "    for ind in range(4):\n",
    "        # attended and regular\n",
    "        mid_att.append(conditions_att[ind])\n",
    "        mid_pred.append(conditions_pred[ind])\n",
    "        # General\n",
    "        # Pick channel\n",
    "        gen_selected_up = subj_uplist[i][conditions[ind]].copy().pick(['POz']) \n",
    "        gen_selected_down = subj_downlist[i][conditions[ind]].copy().pick(['POz'])\n",
    "        upgen_select_dict[conditions[ind]] =  gen_selected_up\n",
    "        downgen_select_dict[conditions[ind]] =  gen_selected_down\n",
    "        # Extract data\n",
    "        gen_up_mean_data = gen_selected_up.copy().crop(tmin=gen_tmin,tmax=gen_tmax).data\n",
    "        gen_down_mean_data = gen_selected_down.copy().crop(tmin=gen_tmin,tmax=gen_tmax).data\n",
    "        # Agragate and append\n",
    "        genup_mean = gen_up_mean_data.mean(axis=1) * 1e6\n",
    "        gendown_mean = gen_down_mean_data.mean(axis=1) * 1e6\n",
    "        genup_amp.append(genup_mean[0])\n",
    "        gendown_amp.append(gendown_mean[0])\n",
    "        # RMS\n",
    "        gen_mean = np.mean([np.sqrt(genup_mean**2),np.sqrt(gendown_mean**2)])\n",
    "        general_amp.append(gen_mean)\n",
    "\n",
    "        # Tailored\n",
    "        # Pick channel\n",
    "        tail_selected_up = subj_uplist[i][conditions[ind]].copy().pick([tailored_channel]) \n",
    "        tail_selected_down = subj_downlist[i][conditions[ind]].copy().pick([tailored_channel])\n",
    "        uptail_select_dict[conditions[ind]] =  tail_selected_up\n",
    "        downtail_select_dict[conditions[ind]] =  tail_selected_down\n",
    "        # Extract data\n",
    "        tail_up_mean_data = tail_selected_up.copy().crop(tmin=tail_tmin,tmax=tail_tmax).data\n",
    "        tail_down_mean_data = tail_selected_down.copy().crop(tmin=tail_tmin,tmax=tail_tmax).data\n",
    "        # Agragate and append\n",
    "        tailup_mean = tail_up_mean_data.mean(axis=1) * 1e6\n",
    "        taildown_mean = tail_down_mean_data.mean(axis=1) * 1e6\n",
    "        tailup_amp.append(tailup_mean[0])\n",
    "        taildown_amp.append(taildown_mean[0])\n",
    "        # RMS\n",
    "        tail_mean = np.mean([np.sqrt(tailup_mean**2),np.sqrt(taildown_mean**2)])\n",
    "        tailored_amp.append(tail_mean)\n",
    "\n",
    "        # Ultra tailored\n",
    "        # Pick channel\n",
    "        ultra_selected_up = subj_uplist[i][conditions[ind]].copy().pick([ultraup_channel]) \n",
    "        ultra_selected_down = subj_downlist[i][conditions[ind]].copy().pick([ultradown_channel])\n",
    "        upultra_select_dict[conditions[ind]] =  ultra_selected_up\n",
    "        downultra_select_dict[conditions[ind]] =  ultra_selected_down\n",
    "        # Extract data\n",
    "        ultra_up_mean_data = ultra_selected_up.copy().crop(tmin=ultraup_tmin,tmax=ultraup_tmax).data\n",
    "        ultra_down_mean_data = ultra_selected_down.copy().crop(tmin=ultradown_tmin,tmax=ultradown_tmax).data\n",
    "        # Agragate and append\n",
    "        ultraup_mean = ultra_up_mean_data.mean(axis=1) * 1e6\n",
    "        ultradown_mean = ultra_down_mean_data.mean(axis=1) * 1e6\n",
    "        ultraup_amp.append(ultraup_mean[0])\n",
    "        ultradown_amp.append(ultradown_mean[0])\n",
    "        # RMS\n",
    "        ultra_mean = np.mean([np.sqrt(ultraup_mean**2),np.sqrt(ultradown_mean**2)])\n",
    "        ultra_amp.append(ultra_mean)\n",
    "    \n",
    "    selectup_gen_evoked.append(upgen_select_dict)\n",
    "    selectdown_gen_evoked.append(downgen_select_dict) \n",
    "    selectup_tail_evoked.append(uptail_select_dict)\n",
    "    selectdown_tail_evoked.append(downtail_select_dict) \n",
    "    selectup_ultra_evoked.append(uptail_select_dict)\n",
    "    selectdown_ultra_evoked.append(downtail_select_dict) \n",
    "\n",
    "c1_long_df['attention'] = mid_att\n",
    "c1_long_df['expectation'] = mid_pred\n",
    "c1_long_df['general_amp'] = general_amp\n",
    "c1_long_df['general_up_amp'] = genup_amp\n",
    "c1_long_df['general_down_amp'] = gendown_amp\n",
    "c1_long_df['tailored_amp'] = tailored_amp\n",
    "c1_long_df['tailored_up_amp'] = tailup_amp\n",
    "c1_long_df['tailored_down_amp'] = taildown_amp\n",
    "c1_long_df['ultra_amp'] = ultra_amp\n",
    "c1_long_df['ultra_up_amp'] = ultraup_amp\n",
    "c1_long_df['ultra_down_amp'] = ultradown_amp\n",
    "c1_long_df['ultra_up_tmin'] = ultraup_tmins\n",
    "c1_long_df['ultra_up_tmax'] = ultraup_tmaxs\n",
    "c1_long_df['ultra_down_tmin'] = ultradown_tmins\n",
    "c1_long_df['ultra_down_tmax'] = ultradown_tmaxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct dataframe for subjects 8 and 9 having truncated datasets (lost trials)\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 9) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'odd'), \n",
    "       ['general_up_amp', 'tailored_up_amp', 'ultra_up_amp','general_amp','tailored_amp','ultra_amp']] = np.nan\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 9) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'regular'), \n",
    "       ['general_down_amp', 'tailored_down_amp', 'ultra_down_amp','general_amp','tailored_amp','ultra_amp']] = np.nan\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 8) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'regular'), \n",
    "       ['general_up_amp', 'tailored_up_amp', 'ultra_up_amp','general_amp','tailored_amp','ultra_amp']] = np.nan\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 8) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'odd'), \n",
    "       ['general_down_amp', 'tailored_down_amp', 'ultra_down_amp','general_amp','tailored_amp','ultra_amp']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvmigem\\AppData\\Local\\Temp\\ipykernel_16672\\2383287542.py:30: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n"
     ]
    }
   ],
   "source": [
    "scale = [-12,12]\n",
    "att_reg_up_data = [] \n",
    "att_odd_up_data = [] \n",
    "unatt_reg_up_data = [] \n",
    "unatt_odd_up_data = [] \n",
    "\n",
    "att_reg_down_data = [] \n",
    "att_odd_down_data = [] \n",
    "unatt_reg_down_data = [] \n",
    "unatt_odd_down_data = []\n",
    "\n",
    "up_data = [att_reg_up_data,att_odd_up_data,unatt_reg_up_data,unatt_odd_up_data]\n",
    "down_data = [att_reg_down_data,att_odd_down_data,unatt_reg_down_data,unatt_odd_down_data]\n",
    "\n",
    "norm_att_reg_up_data = [] \n",
    "norm_att_odd_up_data = [] \n",
    "norm_unatt_reg_up_data = [] \n",
    "norm_unatt_odd_up_data = [] \n",
    "\n",
    "norm_att_reg_down_data = [] \n",
    "norm_att_odd_down_data = [] \n",
    "norm_unatt_reg_down_data = [] \n",
    "norm_unatt_odd_down_data = [] \n",
    "\n",
    "norm_up_data = [norm_att_reg_up_data,norm_att_odd_up_data,norm_unatt_reg_up_data,norm_unatt_odd_up_data]\n",
    "norm_down_data = [norm_att_reg_down_data,norm_att_odd_down_data,norm_unatt_reg_down_data,norm_unatt_odd_down_data]\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    # init fig\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    # take times from single evoked (should be all the same)\n",
    "    times = evoked_pos1_list[0].times\n",
    "    \n",
    "    the_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    posup = sub_pos_info[i]['up_pos']\n",
    "    posdown = sub_pos_info[i]['down_pos']\n",
    "    channel_up = the_row[f'pos{posup}_peak_channel'].iloc[0]\n",
    "    channel_down = the_row[f'pos{posdown}_peak_channel'].iloc[0]\n",
    "    norm_channel = the_row[f'all_pos_peak_channel'].iloc[0]\n",
    " \n",
    "    ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "    evokeds_up_list = [evokeds_up['reg_att'][i],\n",
    "                       evokeds_up['odd_att'][i],\n",
    "                       evokeds_up['reg_unatt'][i],\n",
    "                       evokeds_up['odd_unatt'][i]]\n",
    "    evokeds_down_list = [evokeds_down['reg_att'][i],\n",
    "                         evokeds_down['odd_att'][i],\n",
    "                         evokeds_down['reg_unatt'][i],\n",
    "                         evokeds_down['odd_unatt'][i]]\n",
    "    for idx in range(4):\n",
    "        up = evokeds_up_list[idx].copy().pick(channel_up).data.squeeze() * 1e6\n",
    "        up_data[idx].append(up)\n",
    "        norm_up_data[idx].append(evokeds_up_list[idx].copy().pick(norm_channel).data.squeeze() * 1e6)\n",
    "        sns.lineplot(x=times, y=up, ax = axes[0], label = ev_names[idx])\n",
    "        down = evokeds_down_list[idx].copy().pick(channel_down).data.squeeze() * 1e6\n",
    "        down_data[idx].append(down)\n",
    "        norm_down_data[idx].append(evokeds_down_list[idx].copy().pick(norm_channel).data.squeeze() * 1e6)\n",
    "        sns.lineplot(x=times, y=down, ax = axes[1],label = ev_names[idx])\n",
    "\n",
    "    # mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "    the_row = c1_long_df[c1_long_df['subject'] == sub_pos_info[i]['subject']]\n",
    "    up_tmin= the_row['ultra_up_tmin'].iloc[0]\n",
    "    up_tmax= the_row['ultra_up_tmax'].iloc[0]\n",
    "    down_tmin= the_row['ultra_down_tmin'].iloc[0]\n",
    "    down_tmax= the_row['ultra_down_tmax'].iloc[0]\n",
    "\n",
    "    axes[0].axhline(y=0, lw=1, c='black' )\n",
    "    axes[1].axhline(y=0, lw=1, c='black' )\n",
    "    axes[0].axvline(x=up_tmin,ls='--',lw=1, c='black' )\n",
    "    axes[0].axvline(x=up_tmax,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=down_tmin,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=down_tmax,ls='--',lw=1, c='black' )\n",
    "    axes[0].set_title('Up')\n",
    "    axes[1].set_title('Down')\n",
    "    axes[0].set_ylabel('µV')\n",
    "    axes[0].set_xlabel('time (s)')\n",
    "    axes[1].set_xlabel('time (s)')\n",
    "\n",
    "    plt.title(f\"subject {sub}\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [-12,12]\n",
    "for i,sub in enumerate(sub_list):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    pos = sub_pos_info[i]['down_pos']\n",
    "    channel = the_row[f'pos{pos}_peak_channel'].iloc[0]\n",
    "     \n",
    "    ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "    evokeds_list = [evokeds_down['reg_att'][i],\n",
    "                    evokeds_down['odd_att'][i],\n",
    "                    evokeds_down['reg_unatt'][i],\n",
    "                    evokeds_down['odd_unatt'][i]]\n",
    "    \n",
    "    # conds = ('seq1','seq2','seq3','seq4')\n",
    "    norm = dict(zip(ev_names, evokeds_list))\n",
    "\n",
    "    mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[0], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    mne.viz.plot_compare_evokeds(norm, picks=channel, axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gon make a ver loong dataframe\n",
    "c1_up_df = c1_long_df[['subject','attention','expectation','general_up_amp','tailored_up_amp','ultra_up_amp']]\n",
    "c1_down_df = c1_long_df[['subject','attention','expectation','general_down_amp','tailored_down_amp','ultra_down_amp']]\n",
    "c1_up_df['visual_field'] = 'up'\n",
    "c1_down_df['visual_field'] = 'down'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_up_df = c1_up_df.rename(columns={'general_up_amp':'general_amp', 'tailored_up_amp':'tailored_amp', 'ultra_up_amp': 'ultra_amp'})\n",
    "c1_down_df = c1_down_df.rename(columns={'general_down_amp':'general_amp', 'tailored_down_amp':'tailored_amp', 'ultra_down_amp': 'ultra_amp'})\n",
    "c1_very_long_df = pd.concat([c1_up_df,c1_down_df],axis = 0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files with latencies based on localiser and 30 ms window\n",
    "c1_very_long_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_long_df.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files with latencies based on exp grand average and 30 ms window\n",
    "c1_very_long_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_galat_long_df.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evoked_ga_up = [mne.grand_average(evokeds_up['reg_att']),\n",
    "               mne.grand_average(evokeds_up['odd_att']),\n",
    "               mne.grand_average(evokeds_up['reg_unatt']),\n",
    "               mne.grand_average(evokeds_up['odd_unatt'])]\n",
    "\n",
    "evoked_ga_down = [mne.grand_average(evokeds_down['reg_att']),\n",
    "               mne.grand_average(evokeds_down['odd_att']),\n",
    "               mne.grand_average(evokeds_down['reg_unatt']),\n",
    "               mne.grand_average(evokeds_down['odd_unatt'])]\n",
    "\n",
    "evoked_ga = []\n",
    "\n",
    "for i,(up,down) in enumerate(zip(evoked_ga_up,evoked_ga_down)):\n",
    "    rms_data = np.mean([np.sqrt(up.data**2),np.sqrt(down.data**2)],axis=0)\n",
    "    rms_evoked = mne.EvokedArray(rms_data,info=up.info, tmin=up.times[0])\n",
    "    evoked_ga.append(rms_evoked)\n",
    "\n",
    "conditions = ('attended reg','attended odd','unattended reg','unattended odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_array = np.array(up_data)\n",
    "down_array = np.array(down_data)\n",
    "norm_up_array = np.array(norm_up_data)\n",
    "norm_down_array = np.array(norm_down_data)\n",
    "# drop bad spans\n",
    "up_array[0,6,:] = np.nan\n",
    "down_array[1,6,:] = np.nan\n",
    "up_array[1,7,:] = np.nan\n",
    "down_array[0,7,:] = np.nan\n",
    "norm_up_array[0,6,:] = np.nan\n",
    "norm_down_array[1,6,:] = np.nan\n",
    "norm_up_array[1,7,:] = np.nan\n",
    "norm_down_array[0,7,:] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_total = np.nanmean(up_array,axis=1)\n",
    "down_total = np.nanmean(down_array,axis=1)\n",
    "norm_up_total = np.nanmean(norm_up_array,axis=1)\n",
    "norm_down_total = np.nanmean(norm_down_array,axis=1)\n",
    "\n",
    "\n",
    "# Flatten the array (i.e., stack all the data into one column)\n",
    "flat_norm_up = norm_up_array.flatten()\n",
    "flat_norm_down = norm_down_array.flatten()\n",
    "flat_norm = np.concatenate((flat_norm_up,flat_norm_down))\n",
    "times = evoked_pos1_list[0].times\n",
    "# Create arrays for position, subject, and timepoint\n",
    "rep_conds = np.repeat(conditions, 23 * 155)  # Repeat each position 23 * 155 times\n",
    "subjects = np.tile(np.repeat(sub_list, 155), 4)  # Repeat each subject 155 times for each position\n",
    "timepoints = np.tile(times, 4 * 23)  \n",
    "\n",
    "# Create the DataFrame\n",
    "up_plot_df = pd.DataFrame({\n",
    "    'subject': subjects,\n",
    "    'times': timepoints,\n",
    "    'condition': rep_conds,\n",
    "    'amp': flat_norm_up\n",
    "})\n",
    "# Create the DataFrame\n",
    "down_plot_df = pd.DataFrame({\n",
    "    'subject': subjects,\n",
    "    'times': timepoints,\n",
    "    'condition': rep_conds,\n",
    "    'amp': flat_norm_down\n",
    "})\n",
    "\n",
    "vf_field = np.repeat(['up', 'down'], 4 *23 * 155) \n",
    "# Create the DataFrame\n",
    "plot_df = pd.DataFrame({\n",
    "    'subject': np.tile(subjects,2),\n",
    "    'times': np.tile(timepoints,2),\n",
    "    'condition': np.tile(rep_conds,2),\n",
    "    'visual_field':vf_field,\n",
    "    'amp': flat_norm\n",
    "})\n",
    "\n",
    "plot_df['rms_amp'] = np.sqrt(plot_df['amp']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init fig\n",
    "fig, axes = plt.subplots(2,3, figsize=(12, 6), sharey=True)\n",
    "times = evoked_pos1_list[0].times\n",
    "\n",
    "for idx in range(4):\n",
    "    up_ga = evoked_ga_up[idx].get_data(['POz']).squeeze() * 1e6\n",
    "    sns.lineplot(x=times, y=up_ga, ax = axes[0,0], label = ev_names[idx])\n",
    "    norm_up = norm_up_total[idx,:]\n",
    "    sns.lineplot(x=times, y=norm_up, ax = axes[0,1], label = ev_names[idx])\n",
    "    up = up_total[idx,:]\n",
    "    sns.lineplot(x=times, y=up, ax = axes[0,2], label = ev_names[idx])\n",
    "\n",
    "    down_ga = evoked_ga_down[idx].get_data(['POz']).squeeze() * 1e6\n",
    "    sns.lineplot(x=times, y=down_ga, ax = axes[1,0],label = ev_names[idx])\n",
    "    norm_down = norm_down_total[idx,:]\n",
    "    sns.lineplot(x=times, y=norm_down, ax = axes[1,1],label = ev_names[idx])\n",
    "    down = down_total[idx,:]\n",
    "    sns.lineplot(x=times, y=down, ax = axes[1,2],label = ev_names[idx])\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.axhline(y=0, lw=1, c='black' )\n",
    "    ax.axhline(y=-4, ls ='--',lw=1, c='black' )\n",
    "    ax.axhline(y=4, ls ='--',lw=1, c='black' )\n",
    "    ax.axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    ax.axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "sns.set_context(\"paper\")\n",
    "axes[0,0].set_title('Up POz')\n",
    "axes[0,1].set_title('Up variable')\n",
    "axes[0,2].set_title('Up very variable')\n",
    "axes[1,0].set_title('Down POz')\n",
    "axes[1,1].set_title('Down variable')\n",
    "axes[1,2].set_title('Down very variable')\n",
    "axes[0,0].set_ylabel('µV')\n",
    "axes[1,0].set_ylabel('µV')\n",
    "axes[1,0].set_xlabel('time (s)')\n",
    "axes[1,1].set_xlabel('time (s)')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make rms plots for tailored evokeds\n",
    "rms_total = np.mean([np.sqrt(norm_up_total**2),np.sqrt(norm_down_total**2)],axis=0)\n",
    "rmsrms = np.sqrt(rms_total**2)\n",
    "fig, axes = plt.subplots()\n",
    "times = evoked_pos1_list[0].times\n",
    "handles = ['Regular Attended','Odd Attended','Regular Unattented','Odd Unattended']\n",
    "\n",
    "for idx in range(4):\n",
    "    gwa = rms_total[idx,:]\n",
    "    sns.lineplot(x=times, y=gwa, label = handles[idx])\n",
    "\n",
    "axes.axhline(y=0, lw=1, c='black' )\n",
    "axes.axhline(y=-4, ls ='--',lw=1, c='black' )\n",
    "axes.axhline(y=4, ls ='--',lw=1, c='black' )\n",
    "axes.axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "axes.axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "\n",
    "axes.set_title('Ultra RMS')\n",
    "axes.set_ylabel('µV')\n",
    "axes.set_xlabel('time (s)')\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(16, 10), sharey=True, facecolor ='#C1E5F5')\n",
    "\n",
    "times = evoked_pos1_list[0].times\n",
    "handles = ['Regular Attended','Odd Attended','Regular Unattented','Odd Unattended']\n",
    "line_colours = {'attended reg':my_palette[1],'attended odd':my_palette[1],'unattended reg':my_palette[2],'unattended odd':my_palette[2]}\n",
    "\n",
    "att_up_vf = mlines.Line2D([], [], color=my_palette[1], linestyle='-', label='Attended Regular',linewidth=6)\n",
    "unatt_up_vf = mlines.Line2D([], [], color=my_palette[1], linestyle='dotted', label='Attended Odd',linewidth=6)\n",
    "att_down_vf = mlines.Line2D([], [], color= my_palette[2], linestyle='-', label='Unattented Regular',linewidth=6)\n",
    "unatt_down_vf = mlines.Line2D([], [], color=my_palette[2], linestyle='dotted', label='Unattended Odd',linewidth=6)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "sns.lineplot(data=plot_df, x='times', y='rms_amp',hue='condition',\n",
    "             palette = line_colours,ax=axes, linewidth=6,\n",
    "             errorbar = 'se')\n",
    "# Might need to loop through the list if there are multiple lines on the plot\n",
    "axes.lines[1].set_linestyle(\"dotted\")\n",
    "axes.lines[3].set_linestyle(\"dotted\")\n",
    "\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "plt.rcParams['font.weight'] = 'semibold'\n",
    "legend = plt.legend(handles=[att_up_vf, unatt_up_vf,att_down_vf,unatt_down_vf,],\n",
    "           loc='upper left', bbox_to_anchor=(-.01, 1),prop={'size':22},facecolor= 'white')\n",
    "\n",
    "plt.gca().set_facecolor('#C1E5F5')\n",
    "sns.despine(offset=10, trim=True);\n",
    "for i in range(1):\n",
    "    axes.axhline(y=0, lw=1, c='black' )\n",
    "    axes.axhline(y=4, ls ='--',lw=1, c='black' )\n",
    "    axes.axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    axes.axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "    axes.set_ylabel('µV (absolute value)', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "    axes.set_xlabel('time (s)',fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "\n",
    "sns.axes_style(\"ticks\")\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(16, 6), sharey=True, facecolor ='#C1E5F5')\n",
    "\n",
    "c1_very_long_df['rms_tailored'] = np.sqrt(c1_very_long_df['tailored_amp']**2)\n",
    "\n",
    "att_label = mlines.Line2D([], [], color=my_palette[4], linestyle='-', label='Attended',linewidth=6)\n",
    "unatt_label = mlines.Line2D([], [], color=my_palette[8], linestyle='-', label='Unattended',linewidth=6)\n",
    "reg_label = mlines.Line2D([], [], color= my_palette[9], linestyle='-', label='Regular',linewidth=6)\n",
    "odd_label = mlines.Line2D([], [], color=my_palette[6], linestyle='-', label='Odd',linewidth=6)\n",
    "\n",
    "sns.barplot(c1_very_long_df, y= 'rms_tailored', x= 'visual_field', hue= 'attention', edgecolor= 'none', palette= [my_palette[4],my_palette[8]],\n",
    "            errorbar='se', errwidth= 5,\n",
    "            capsize=.1,width=.4, linewidth=0.7, ax=axes[0])\n",
    "\n",
    "sns.barplot(c1_very_long_df , y= 'rms_tailored', x= 'visual_field', hue= 'expectation', edgecolor= 'none', palette= [my_palette[9],my_palette[6]],\n",
    "            errorbar='se', errwidth= 5,\n",
    "            capsize=.1,width=.4, linewidth=0.7, ax=axes[1])\n",
    "\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "plt.rcParams['font.weight'] = 'semibold'\n",
    "sns.set_context(\"poster\")\n",
    "axes[0].set_ylim(0,5)\n",
    "axes[0].set_facecolor('#C1E5F5')\n",
    "axes[1].set_facecolor('#C1E5F5')\n",
    "axes[0].legend(handles=[att_label,unatt_label],\n",
    "           loc='upper left', bbox_to_anchor=(.65, 1),prop={'size':22},facecolor= 'white')\n",
    "axes[1].legend(handles=[reg_label,odd_label],\n",
    "           loc='upper left', bbox_to_anchor=(.65, 1),prop={'size':22},facecolor= 'white')\n",
    "\n",
    "axes[0].set_ylabel('µV (absolute value)', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "axes[0].set_xlabel('Visual Field', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "axes[1].set_xlabel('Visual Field', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "\n",
    "sns.despine(offset=10, trim=True);\n",
    "sns.axes_style(\"ticks\")\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, figsize=(16, 10), sharey=True, facecolor ='#C1E5F5')\n",
    "\n",
    "c1_very_long_df['rms_tailored'] = np.sqrt(c1_very_long_df['tailored_amp']**2)\n",
    "\n",
    "att_label = mlines.Line2D([], [], color=my_palette[4], linestyle='-', label='Attended',linewidth=6)\n",
    "unatt_label = mlines.Line2D([], [], color=my_palette[8], linestyle='-', label='Unattended',linewidth=6)\n",
    "reg_label = mlines.Line2D([], [], color= my_palette[9], linestyle='-', label='Regular',linewidth=6)\n",
    "odd_label = mlines.Line2D([], [], color=my_palette[6], linestyle='-', label='Odd',linewidth=6)\n",
    "\n",
    "bar_plot= sns.barplot(p3_df, y= 'mean_amp', x= 'expectation', hue= 'attention', palette= [my_palette[4],my_palette[8]],\n",
    "            errorbar='se', errwidth= 7,\n",
    "            capsize=.1,width=.4, linewidth=0.7)\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "plt.rcParams['font.weight'] = 'semibold'\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "axes.set_facecolor('#C1E5F5')\n",
    "\n",
    "axes.legend(handles=[att_label,unatt_label],\n",
    "           loc='upper left', bbox_to_anchor=(.9, 1),prop={'size':36},facecolor= 'white')\n",
    "\n",
    "axes.set_ylim(0,3)\n",
    "axes.set_ylabel('µV', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':48})\n",
    "axes.set_xlabel('Expectation', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':48})\n",
    "\n",
    "bar_plot.spines['left'].set_linewidth(6)   # Set thickness for left spine\n",
    "bar_plot.spines['bottom'].set_linewidth(6)\n",
    "plt.tick_params(axis='both', width=6, length=12,labelsize = 36)\n",
    "\n",
    "sns.despine(offset=10, trim=True);\n",
    "sns.axes_style(\"ticks\")\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailor_up_evokeds['reg_att'].plot(ylim=dict(eeg=scale))\n",
    "tailor_up_evokeds['odd_att'].plot(ylim=dict(eeg=scale))\n",
    "tailor_up_evokeds['reg_unatt'].plot(ylim=dict(eeg=scale))\n",
    "tailor_up_evokeds['odd_unatt'].plot(ylim=dict(eeg=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'C1', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'C2', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Cz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'P1', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'P2', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Pz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'PO3', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'PO4', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'POz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'O1', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'O2', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Oz', vlines=[0.05,0.1],ylim=dict(eeg=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Attention only effects\n",
    "\"\"\"\n",
    "\n",
    "ep_attended = epochs_nocatch['attention == \"attended\"']\n",
    "ep_unattended = epochs_nocatch['attention == \"unattended\"']\n",
    "\n",
    "att_1 = []\n",
    "att_2 = []\n",
    "att_3 = []\n",
    "att_4 = []\n",
    "\n",
    "unatt_1 = []\n",
    "unatt_2 = []\n",
    "unatt_3 = []\n",
    "unatt_4 = []\n",
    "\n",
    "attended_evokeds = [att_1,att_2,att_3,att_4]\n",
    "unattended_evokeds = [unatt_1,unatt_2,unatt_3,unatt_4]\n",
    "\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    for idx in range(4):\n",
    "        att_ev = ep_attended[f'participant == {sub}'][f'pos{idx+1}'].average()\n",
    "        attended_evokeds[idx].append(att_ev)\n",
    "        unatt_ev = ep_unattended[f'participant == {sub}'][f'pos{idx+1}'].average()\n",
    "        unattended_evokeds[idx].append(unatt_ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_up = []\n",
    "att_down = []\n",
    "unatt_up = []\n",
    "unatt_down = []\n",
    "\n",
    "attended_evokeds = [att_up,att_down]\n",
    "unattended_evokeds = [unatt_up,unatt_down]\n",
    "\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "\n",
    "    att_ev_1 = ep_attended[f'participant == {sub}']['pos1']\n",
    "    att_ev_2 = ep_attended[f'participant == {sub}']['pos2']\n",
    "    att_ev_up = mne.concatenate_epochs([att_ev_1, att_ev_2])\n",
    "    attended_evokeds[0].append(att_ev_up.average())\n",
    "\n",
    "    att_ev_3 = ep_attended[f'participant == {sub}']['pos3']\n",
    "    att_ev_4 = ep_attended[f'participant == {sub}']['pos4']\n",
    "    att_ev_down = mne.concatenate_epochs([att_ev_3, att_ev_4])\n",
    "    attended_evokeds[1].append(att_ev_down.average())\n",
    "\n",
    "    unatt_ev_1 = ep_unattended[f'participant == {sub}']['pos1']\n",
    "    unatt_ev_2 = ep_unattended[f'participant == {sub}']['pos2']\n",
    "    unatt_ev_up = mne.concatenate_epochs([unatt_ev_1, unatt_ev_2])\n",
    "    unattended_evokeds[0].append(unatt_ev_up.average())\n",
    "\n",
    "    unatt_ev_3 = ep_unattended[f'participant == {sub}']['pos3']\n",
    "    unatt_ev_4 = ep_unattended[f'participant == {sub}']['pos4']\n",
    "    unatt_ev_down = mne.concatenate_epochs([unatt_ev_3, unatt_ev_4])\n",
    "    unattended_evokeds[1].append(unatt_ev_down.average())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "attention_conds_up = ['att_pos1','att_pos2','unatt_pos1','unatt_pos2',]\n",
    "evoked_ga_attention_up = [mne.grand_average(attended_evokeds[0]),\n",
    "                       mne.grand_average(attended_evokeds[1]),\n",
    "                       mne.grand_average(unattended_evokeds[0]),\n",
    "                       mne.grand_average(unattended_evokeds[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_conds_down = ['att_pos3','att_pos4','unatt_pos3','unatt_pos4']\n",
    "evoked_ga_attention_down = [mne.grand_average(attended_evokeds[2]),\n",
    "                       mne.grand_average(attended_evokeds[3]),\n",
    "                       mne.grand_average(unattended_evokeds[2]),\n",
    "                       mne.grand_average(unattended_evokeds[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_conds_up = ['att_up','unatt_up']\n",
    "evoked_ga_attention_up = [mne.grand_average(attended_evokeds[0]),\n",
    "                       mne.grand_average(unattended_evokeds[0])]\n",
    "attention_conds_down = ['att_down','unatt_down']\n",
    "evoked_ga_attention_down = [mne.grand_average(attended_evokeds[1]),\n",
    "                       mne.grand_average(unattended_evokeds[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "norm1 = dict(zip(attention_conds_up,evoked_ga_attention_up))\n",
    "norm2 = dict(zip(attention_conds_down,evoked_ga_attention_down))\n",
    "\n",
    "scale = [-6, 6]\n",
    "\n",
    "mne.viz.plot_compare_evokeds(norm1,picks=['POz'],vlines=[0.05,0.1],ylim=dict(eeg=scale),axes=axes[0])\n",
    "mne.viz.plot_compare_evokeds(norm2,picks=['POz'],vlines=[0.05,0.1],ylim=dict(eeg=scale),axes=axes[1])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "times = np.linspace(0.1, 0.5, 7)\n",
    "vlim=(-3, 3)\n",
    "\n",
    "evoked_pred[0].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)\n",
    "evoked_pred[1].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)\n",
    "evoked_pred[2].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)\n",
    "evoked_pred[3].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time course of c1 peaks and means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Time course first sequence position\n",
    "\"\"\"\n",
    "# attended\n",
    "ep_df_att_1 = ep_att_1.metadata\n",
    "ep_df_att_1['poz_c1_mean'] = ep_att_1.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_att_2 = ep_att_2.metadata\n",
    "ep_df_att_2['poz_c1_mean'] = ep_att_2.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_att_3 = ep_att_3.metadata\n",
    "ep_df_att_3['poz_c1_mean'] = ep_att_3.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_att_4 = ep_att_4.metadata\n",
    "ep_df_att_4['poz_c1_mean'] = ep_att_4.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "# unattend\n",
    "ep_df_unatt_1= ep_unatt_1.metadata\n",
    "ep_df_unatt_1['poz_c1_mean'] = ep_unatt_1.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_unatt_2 = ep_unatt_2.metadata\n",
    "ep_df_unatt_2['poz_c1_mean'] = ep_unatt_2.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_unatt_3 = ep_unatt_3.metadata\n",
    "ep_df_unatt_3['poz_c1_mean'] = ep_unatt_3.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_unatt_4 = ep_unatt_4.metadata\n",
    "ep_df_unatt_4['poz_c1_mean'] = ep_unatt_4.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Time course\n",
    "\"\"\"\n",
    "position = ['pos1','pos2','pos3','pos4']\n",
    "condition = 'attention == \"attended\" & expected == \"regular\" & start_position == 2'\n",
    "pos1_win_data = c1_window_epochs[condition][position[0]].get_data(picks='POz')\n",
    "pos2_win_data = c1_window_epochs[condition][position[1]].get_data(picks='POz')\n",
    "pos3_win_data = c1_window_epochs[condition][position[2]].get_data(picks='POz')\n",
    "pos4_win_data = c1_window_epochs[condition][position[3]].get_data(picks='POz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_c1_1 = pos1_win_data.mean(axis=2).squeeze()\n",
    "mean_c1_2 = pos2_win_data.mean(axis=2).squeeze()\n",
    "mean_c1_3 = pos3_win_data.mean(axis=2).squeeze()\n",
    "mean_c1_4 = pos4_win_data.mean(axis=2).squeeze()\n",
    "\n",
    "max_c1_1 = pos1_win_data.max(axis=2).squeeze()\n",
    "max_c1_2 = pos2_win_data.max(axis=2).squeeze()\n",
    "max_c1_3 = pos3_win_data.max(axis=2).squeeze()\n",
    "max_c1_4 = pos4_win_data.max(axis=2).squeeze()\n",
    "\n",
    "mean_c1_list = [mean_c1_1,mean_c1_2,mean_c1_3,mean_c1_4]\n",
    "max_c1_list = [max_c1_1,max_c1_2,max_c1_3,max_c1_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting mean c1 voltage time wise\n",
    "\"\"\"\n",
    "plt.rcParams[\"figure.figsize\"] = [18,10]\n",
    "fig, axs = plt.subplots(4,2, sharey=True)\n",
    "\n",
    "sns.regplot(data = ep_df_att_1, x = 'trial', y = 'poz_c1_mean', ax = axs[0,0],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_1, x = 'trial', y = 'poz_c1_mean', ax = axs[1,0],scatter_kws={'s': 10, 'color': 'green'})\n",
    "sns.regplot(data = ep_df_att_2, x = 'trial', y = 'poz_c1_mean', ax = axs[0,1],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_2, x = 'trial', y = 'poz_c1_mean', ax = axs[1,1],scatter_kws={'s': 10, 'color': 'green'})\n",
    "sns.regplot(data = ep_df_att_3, x = 'trial', y = 'poz_c1_mean', ax = axs[2,1],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_3, x = 'trial', y = 'poz_c1_mean', ax = axs[3,1],scatter_kws={'s': 10, 'color': 'green'})\n",
    "sns.regplot(data = ep_df_att_4, x = 'trial', y = 'poz_c1_mean', ax = axs[2,0],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_4, x = 'trial', y = 'poz_c1_mean', ax = axs[3,0],scatter_kws={'s': 10, 'color': 'green'})\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# sns.regplot(x = np.arange(len(mean_c1_list[1])), y = mean_c1_list[1], ax=axs[0,1])\n",
    "# sns.regplot(x = np.arange(len(mean_c1_list[2])), y = mean_c1_list[2], ax=axs[1,1])\n",
    "# sns.regplot(x = np.arange(len(mean_c1_list[3])), y = mean_c1_list[3], ax=axs[1,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting max c1 voltage time wise\n",
    "\"\"\"\n",
    "plt.rcParams[\"figure.figsize\"] = [18,10]\n",
    "fig, axs = plt.subplots(2,2, sharey=True)\n",
    "\n",
    "sns.regplot(x = np.arange(len(max_c1_list[0])), y = max_c1_list[0], ax=axs[0,0])\n",
    "sns.regplot(x = np.arange(len(max_c1_list[1])), y = max_c1_list[1], ax=axs[0,1])\n",
    "sns.regplot(x = np.arange(len(max_c1_list[2])), y = max_c1_list[2], ax=axs[1,1])\n",
    "sns.regplot(x = np.arange(len(max_c1_list[3])), y = max_c1_list[3], ax=axs[1,0])\n",
    "\n",
    "plt.suptitle('max values '+ condition)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time frequency exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_first_att = epochs_attended['pos4/seq2'].average().compute_psd()\n",
    "spec_last_att = epochs_attended['seq3'].average().compute_psd()\n",
    "spec_first_unatt = epochs_unattended['seq1'].average().compute_psd()\n",
    "spec_last_unatt = epochs_unattended['seq3'].average().compute_psd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_first_att.plot_topomap(ch_type=\"eeg\")\n",
    "spec_first_unatt.plot_topomap(ch_type=\"eeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.logspace(*np.log10([8, 13]), num=8)\n",
    "n_cycles = freqs / 3.0  # different number of cycle per frequency\n",
    "power, itc = tfr_morlet(\n",
    "    epochs_attended['pos4/seq2'],\n",
    "    freqs=freqs,\n",
    "    n_cycles=n_cycles,\n",
    "    use_fft=True,\n",
    "    return_itc=True,\n",
    "    n_jobs=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "power.plot([27], baseline=(-0.09, 0), mode=\"logratio\", title=power.ch_names[27])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7, 4), layout=\"constrained\")\n",
    "topomap_kw = dict(\n",
    "    ch_type=\"eeg\", tmin=0.09, tmax=0.5, baseline=(-0.09, 0), mode=\"logratio\", show=False\n",
    ")\n",
    "plot_dict = dict(Alpha=dict(fmin=8, fmax=13))\n",
    "for ax, (title, fmin_fmax) in zip(axes, plot_dict.items()):\n",
    "    power.plot_topomap(**fmin_fmax, axes=ax, **topomap_kw)\n",
    "    ax.set_title(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
