{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne, os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "***Aestetics***\n",
    "\"\"\"\n",
    "# load color palette\n",
    "my_palette = sns.color_palette().as_hex()\n",
    "sns.color_palette().as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load data\n",
    "\"\"\"\n",
    "# Big list\n",
    "ep_list = []\n",
    "# # Sup grouped list\n",
    "# ep_group1 = []\n",
    "# ep_group2 = []\n",
    "# ep_group3 = []\n",
    "# ep_group4 = []\n",
    "# subjects = []\n",
    "# Specify the epoch length we will be looking at\n",
    "epoch_tmin = -0.1\n",
    "epoch_tmax = 0.46\n",
    "\n",
    "cleaned_data_dir = '/Users/mvmigem/Documents/data/project_1/preprocessed/mastoid_ref/'\n",
    "dir_list = glob.glob(cleaned_data_dir+'*-epo.fif')\n",
    "excuded_pp = [3,14,20]\n",
    "\n",
    "# Iterate throug data folders \n",
    "for i,sub_path in enumerate(dir_list):\n",
    "    sub = int(sub_path.split('main_eventset_mastoidref_')[1].split('-epo.fif')[0])\n",
    "    if sub in excuded_pp:\n",
    "        continue\n",
    "    clean_epoch_path = sub_path\n",
    "    epochs = mne.read_epochs(clean_epoch_path)\n",
    "    epochs.info['bads']= []\n",
    "    ep_list.append(epochs)\n",
    "\n",
    "    # if epochs.metadata['loc_quad'].iloc[0] == 0:\n",
    "    #     ep_group1.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 1:\n",
    "    #     ep_group2.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 2:\n",
    "    #     ep_group3.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 3:\n",
    "    #     ep_group4.append(epochs)\n",
    "    print(epochs.info['nchan'])\n",
    "\n",
    "# Concat list into big epoch object with all data\n",
    "eps = mne.concatenate_epochs(ep_list).crop(tmin=epoch_tmin,tmax=epoch_tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Subselections\n",
    "\"\"\"\n",
    "# Subject list\n",
    "sub_list = eps.metadata['participant'].unique() \n",
    "\n",
    "# Divide the epoch file into sections based on metadata that can't be distinguished by event names\n",
    "# Drop catch trials\n",
    "epochs_nocatch = eps.copy()['catch_trial == 0']\n",
    "\n",
    "# Add events to metadata\n",
    "metadata = epochs_nocatch.metadata\n",
    "events = epochs_nocatch.events\n",
    "\n",
    "metadata['events'] = events[:,2]\n",
    "\n",
    "epochs_nocatch.metadata = metadata\n",
    "\n",
    "# # Divide attention conditions\n",
    "# epochs_attended = epochs_nocatch['attention == \"attended\"']\n",
    "# epochs_unattended = epochs_nocatch['attention == \"unattended\"']\n",
    "# # Divide by staring position\n",
    "# epochs_start3 = epochs_nocatch['start_position == 2']\n",
    "# epochs_start1 = epochs_nocatch['start_position == 0']\n",
    "# epochs_start2 = epochs_nocatch['start_position == 1']\n",
    "# epochs_start4 = epochs_nocatch['start_position == 3']\n",
    "# # Divide unattended trials by start pos\n",
    "# epochs_unattended_start3 = epochs_unattended['start_position == 2']\n",
    "# epochs_unattended_start1 = epochs_unattended['start_position == 0']\n",
    "# epochs_unattended_start2 = epochs_unattended['start_position == 1']\n",
    "# epochs_unattended_start4 = epochs_unattended['start_position == 3']\n",
    "# # Divide attented trials by start pos\n",
    "# epochs_attended_start3 = epochs_attended['start_position == 2']\n",
    "# epochs_attended_start1 = epochs_attended['start_position == 0']\n",
    "# epochs_attended_start2 = epochs_attended['start_position == 1']\n",
    "# epochs_attended_start4 = epochs_attended['start_position == 3']\n",
    "# # Extra devision for the odd boys\n",
    "# epochs_attended_reg = epochs_attended['expected ==\"regular\"']\n",
    "# epochs_attended_odd = epochs_attended['expected == \"odd\"']\n",
    "# epochs_unattended_reg = epochs_unattended['expected ==\"regular\"']\n",
    "# epochs_unattended_odd = epochs_unattended['expected == \"odd\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Position based ERPs\n",
    "\"\"\"\n",
    "evoked_pos1_list = []\n",
    "evoked_pos2_list = []\n",
    "evoked_pos3_list = []\n",
    "evoked_pos4_list = []\n",
    "for i,sub in enumerate(sub_list):\n",
    "    evoked_pos1_list.append(epochs_nocatch['pos1'][f'participant == {sub}'].average())\n",
    "    evoked_pos2_list.append(epochs_nocatch['pos2'][f'participant == {sub}'].average())\n",
    "    evoked_pos3_list.append(epochs_nocatch['pos3'][f'participant == {sub}'].average())\n",
    "    evoked_pos4_list.append(epochs_nocatch['pos4'][f'participant == {sub}'].average())\n",
    "\n",
    "evoked_pos1 = mne.grand_average(evoked_pos1_list)\n",
    "evoked_pos2 = mne.grand_average(evoked_pos2_list)\n",
    "evoked_pos3 = mne.grand_average(evoked_pos3_list)\n",
    "evoked_pos4 = mne.grand_average(evoked_pos4_list)\n",
    "\n",
    "evokeds_list = [evoked_pos1,evoked_pos2,evoked_pos3,evoked_pos4]\n",
    "conds = ('pos1','pos2','pos3','pos4')\n",
    "# conds = ('seq1','seq2','seq3','seq4')\n",
    "\n",
    "norm = dict(zip(conds, evokeds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peak properties of localiser data\n",
    "peak_properties = pd.read_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_peak_properties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous peak_properties dataframe uses peak latencies \n",
    "# based on the localiser data which may not be optimal\n",
    "# so I'm now making another dataframe to select the latencies\n",
    "# of the main experiment\n",
    "\n",
    "evokeds_lists = [evoked_pos1_list,evoked_pos2_list,evoked_pos3_list,evoked_pos4_list]\n",
    "\n",
    "all_pos = []\n",
    "pos1 = []\n",
    "pos2 = []\n",
    "pos3 = []\n",
    "pos4 = []\n",
    "\n",
    "positions = [pos1,pos2,pos3,pos4]\n",
    "peak_variables = ['peak_channel','peak_latency','peak_amplitude','subject']\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub]\n",
    "    av_list = []\n",
    "    for ind,evoked in enumerate(evokeds_lists):\n",
    "        channel = the_row[f'pos{ind+1}_peak_channel'].iloc[0]\n",
    "        roi_ev = evoked[i].copy().pick(channel)\n",
    "        \n",
    "        # mode = 'pos'\n",
    "        # if ind == 0 or ind == 1:\n",
    "        #     mode = 'neg\n",
    "        ch,lat,amp = roi_ev.get_peak(ch_type='eeg',\n",
    "                                    tmin=0.06,tmax=0.09,\n",
    "                                    mode= 'abs',\n",
    "                                    return_amplitude=True)\n",
    "        positions[ind].append(dict(zip(peak_variables,(ch,lat,amp,sub))))\n",
    "\n",
    "        # rms the tailored latencies\n",
    "        ev_data = evoked[i].data\n",
    "        rootsqr_data = np.sqrt((ev_data**2))\n",
    "        rsq_ev = mne.EvokedArray(rootsqr_data,evoked[i].info,tmin=evoked[i].times[0])\n",
    "        av_list.append(rsq_ev)\n",
    "    # also for the tailored latencies\n",
    "    all_pos_channel = the_row['all_pos_peak_channel'].iloc[0]\n",
    "    merged_ev = mne.grand_average(av_list)\n",
    "    ch,lat,amp = merged_ev.get_peak(ch_type='eeg',   \n",
    "                                tmin=0.06,tmax=0.09,\n",
    "                                return_amplitude=True)\n",
    "    all_pos.append(dict(zip(peak_variables,(ch,lat,amp))))\n",
    "\n",
    "main_all_pos = pd.DataFrame(all_pos)\n",
    "main_all_pos.rename(columns={'peak_channel':'all_pos_peak_channel',\n",
    "                             'peak_latency':'all_pos_peak_latency',\n",
    "                             'peak_amplitude':'all_pos_peak_amplitude'},inplace=True)\n",
    "main_pos1 = pd.DataFrame(positions[0])\n",
    "main_pos1.rename(columns={'peak_channel':'pos1_peak_channel',\n",
    "                          'peak_latency':'pos1_peak_latency',\n",
    "                          'peak_amplitude':'pos1_peak_amplitude'},inplace=True)\n",
    "main_pos1.drop(columns=['subject'],inplace=True)\n",
    "main_pos2 = pd.DataFrame(positions[1])\n",
    "main_pos2.rename(columns={'peak_channel':'pos2_peak_channel',\n",
    "                          'peak_latency':'pos2_peak_latency',\n",
    "                          'peak_amplitude':'pos2_peak_amplitude'},inplace=True)\n",
    "main_pos2.drop(columns='subject',inplace=True)\n",
    "main_pos3 = pd.DataFrame(positions[2])\n",
    "main_pos3.rename(columns={'peak_channel':'pos3_peak_channel',\n",
    "                          'peak_latency':'pos3_peak_latency',\n",
    "                          'peak_amplitude':'pos3_peak_amplitude'},inplace=True)\n",
    "main_pos3.drop(columns='subject',inplace=True)\n",
    "main_pos4 = pd.DataFrame(positions[3])\n",
    "main_pos4.rename(columns={'peak_channel':'pos4_peak_channel',\n",
    "                          'peak_latency':'pos4_peak_latency',\n",
    "                          'peak_amplitude':'pos4_peak_amplitude'},inplace=True)\n",
    "main_pos4.drop(columns='subject',inplace=True)\n",
    "general_pos = peak_properties[peak_properties['subject'].isin(sub_list)]\n",
    "\n",
    "general_pos = general_pos[['grand_average_peak_channel',\n",
    "                                'grand_average_peak_latency',\n",
    "                                'grand_average_peak_amplitude',\n",
    "                                'subject']].reset_index(drop=True)\n",
    "\n",
    "main_pos = [main_all_pos,main_pos1,main_pos2,main_pos3,main_pos4,general_pos]\n",
    "main_pos_df = pd.concat(main_pos,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting positions per subject\n",
    "scale = [-12,12]\n",
    "for i,sub in enumerate(sub_list):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub]\n",
    "    channel = the_row['all_pos_peak_channel'].iloc[0]\n",
    "\n",
    "    ev_names = ('pos1','pos2','pos3','pos4')\n",
    "    evokeds_list = [evoked_pos1_list[i],evoked_pos2_list[i],evoked_pos3_list[i],evoked_pos4_list[i]]\n",
    "    # conds = ('seq1','seq2','seq3','seq4')\n",
    "    norm = dict(zip(ev_names, evokeds_list))\n",
    "\n",
    "    mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[0], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    mne.viz.plot_compare_evokeds(norm, picks=channel, axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [-12,12]\n",
    "evokeds_lists = [evoked_pos1_list,evoked_pos2_list,evoked_pos3_list,evoked_pos4_list]\n",
    "for i,sub in enumerate(sub_list):\n",
    "    # init fig\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    # take times from single evoked ( should be all the same)\n",
    "    times = evoked_pos1_list[0].times\n",
    "    # take sub loc peak info\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub]\n",
    "    # iter over positions\n",
    "    for idx in range(4):\n",
    "        standard = evokeds_lists[idx][i].copy().pick('POz').data.squeeze() * 1e6\n",
    "        sns.lineplot(x=times, y=standard, ax = axes[0], label = f'Pos{idx+1} POz')\n",
    "        channel = the_row[f'pos{idx+1}_peak_channel'].iloc[0]\n",
    "        selected = evokeds_lists[idx][i].copy().pick(channel).data.squeeze() * 1e6\n",
    "        sns.lineplot(x=times, y=selected, ax = axes[1],label = f'Pos{idx+1} {channel}')\n",
    "\n",
    "    # mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "    axes[0].axhline(y=0, lw=1, c='black' )\n",
    "    axes[1].axhline(y=0, lw=1, c='black' )\n",
    "    axes[0].axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    axes[0].axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "    axes[0].set_title('Fixed channel')\n",
    "    axes[1].set_title('Variable channel')\n",
    "    axes[0].set_ylabel('µV')\n",
    "    axes[0].set_xlabel('time (s)')\n",
    "    axes[1].set_xlabel('time (s)')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Subselection main analysis (Second stimulus)\n",
    "\"\"\"\n",
    "# Remove catch\n",
    "# epochs_nocatch = eps.copy()['catch_trial == 0']['pos4']\n",
    "# Select second stim of trial\n",
    "epochs_stim2 = epochs_nocatch['seq2']\n",
    "\n",
    "# Add visual_field\n",
    "metadata = epochs_stim2.metadata\n",
    "\n",
    "metadata['visual_field'] = np.where(metadata['events'].isin([12, 22]), 'up', 'down')\n",
    "\n",
    "epochs_stim2.metadata = metadata\n",
    "# Regular vs odd\n",
    "epochs_odd = epochs_stim2['expected == \"odd\"']\n",
    "epochs_reg = epochs_stim2['expected == \"regular\"']['precedes_odd == 1'] # equalizes trials\n",
    "# Attention\n",
    "ep_odd_att = epochs_odd['attention == \"attended\"']\n",
    "ep_odd_unatt = epochs_odd['attention == \"unattended\"']\n",
    "ep_reg_att = epochs_reg['attention == \"attended\"']\n",
    "ep_reg_unatt = epochs_reg['attention == \"unattended\"']\n",
    "# Visual_field\n",
    "ep_odd_att_up = ep_odd_att['visual_field == \"up\"']\n",
    "ep_odd_unatt_up = ep_odd_unatt['visual_field == \"up\"']\n",
    "ep_reg_att_up = ep_reg_att['visual_field == \"up\"']\n",
    "ep_reg_unatt_up = ep_reg_unatt['visual_field == \"up\"']\n",
    "\n",
    "ep_odd_att_down = ep_odd_att['visual_field == \"down\"']\n",
    "ep_odd_unatt_down = ep_odd_unatt['visual_field == \"down\"']\n",
    "ep_reg_att_down = ep_reg_att['visual_field == \"down\"']\n",
    "ep_reg_unatt_down = ep_reg_unatt['visual_field == \"down\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "recombined_epoch = mne.concatenate_epochs([ep_reg_att, ep_odd_att,ep_reg_unatt,ep_odd_unatt])\n",
    "recombined_epoch.save(f\"C:/Users/mvmigem/Documents/data/project_1/uncorrected_epochs/pre_stim_epochs-epo.fif\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Counting the amount of trials for every processing step\n",
    "\"\"\"\n",
    "\n",
    "df_to_count = epochs_reg.metadata\n",
    "# df_to_count = df_to_count[~(df_to_count.index % 5 == 0)] # have to kind of decimate\n",
    "\n",
    "\n",
    "grouped = df_to_count.groupby(['participant', 'attention', 'expected']).size().reset_index(name='row_count')\n",
    "\n",
    "# Step 3: Group by attention and expectation, and calculate the mean and standard deviation\n",
    "stats_per_condition = grouped.groupby(['attention', 'expected'])['row_count'].agg(['mean', 'std']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reat_up = ep_odd_att_up[\"participant == 1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Selection, extraction and export of all channels relevant for uncorrected P1, N1 and P3 analysis\n",
    "\"\"\"\n",
    "att_reg_ev = []\n",
    "att_odd_ev = []\n",
    "unatt_reg_ev = []\n",
    "unatt_odd_ev = []\n",
    "# totals_grand = []\n",
    "\n",
    "destinantion_path = 'C:/Users/mvmigem/Documents/data/project_1/evoked_df/'\n",
    "\n",
    "#,'POz','PO3','PO4','PO8','O2','PO7','O1','P4','P3'\n",
    "for el in ['Pz','POz','PO3','PO4','PO8','O2','PO7','O1','P4','P3']:\n",
    "    spec_path  = os.path.join(destinantion_path, f'{el}')\n",
    "    for i, sub in enumerate(sub_list):\n",
    "        columns = ['attention','expectation','visual_field']\n",
    "        reat_up = ep_reg_att_up[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_1 = pd.DataFrame({\n",
    "                            'subject': sub,\n",
    "                            'time': reat_up.times,\n",
    "                            'attention': 'attended',\n",
    "                            'expectation':'regular',\n",
    "                            'visual_field':'up',\n",
    "                            'yhat': reat_up.data.flatten() * 1e6  \n",
    "                             })\n",
    "        oat_up = ep_odd_att_up[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_2 = pd.DataFrame({\n",
    "                        'subject': sub,\n",
    "                        'time': oat_up.times,\n",
    "                        'attention': 'attended',\n",
    "                        'expectation':'odd',\n",
    "                        'visual_field':'up',\n",
    "                        'yhat': oat_up.data.flatten() * 1e6\n",
    "                        })\n",
    "        reun_up = ep_reg_unatt_up[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_3 = pd.DataFrame({\n",
    "                    'subject': sub,\n",
    "                    'time': reun_up.times,\n",
    "                    'attention': 'unattended',\n",
    "                    'expectation':'regular',\n",
    "                    'visual_field':'up',\n",
    "                    'yhat': reun_up.data.flatten() * 1e6\n",
    "                        })\n",
    "\n",
    "        oun_up = ep_odd_unatt_up[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_4 = pd.DataFrame({\n",
    "                    'subject': sub,\n",
    "                    'time': oun_up.times,\n",
    "                    'attention': 'unattended',\n",
    "                    'expectation':'odd',\n",
    "                    'visual_field':'up',\n",
    "                    'yhat': oun_up.data.flatten() * 1e6\n",
    "                        })\n",
    "        reat_down = ep_reg_att_down[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_5 = pd.DataFrame({\n",
    "                            'subject': sub,\n",
    "                            'time': reat_down.times,\n",
    "                            'attention': 'attended',\n",
    "                            'expectation':'regular',\n",
    "                            'visual_field':'down',\n",
    "                            'yhat': reat_down.data.flatten() * 1e6  \n",
    "                             })\n",
    "        oat_down = ep_odd_att_down[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_6 = pd.DataFrame({\n",
    "                        'subject': sub,\n",
    "                        'time': oat_down.times,\n",
    "                        'attention': 'attended',\n",
    "                        'expectation':'odd',\n",
    "                        'visual_field':'down',\n",
    "                        'yhat': oat_down.data.flatten() * 1e6\n",
    "                        })\n",
    "        reun_down = ep_reg_unatt_down[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_7 = pd.DataFrame({\n",
    "                    'subject': sub,\n",
    "                    'time': reun_down.times,\n",
    "                    'attention': 'unattended',\n",
    "                    'expectation':'regular',\n",
    "                    'visual_field':'down',\n",
    "                    'yhat': reun_down.data.flatten() * 1e6\n",
    "                        })\n",
    "\n",
    "        oun_down = ep_odd_unatt_down[f\"participant == {sub}\"].copy().pick(el).average()\n",
    "        df_8 = pd.DataFrame({\n",
    "                    'subject': sub,\n",
    "                    'time': oun_down.times,\n",
    "                    'attention': 'unattended',\n",
    "                    'expectation':'odd',\n",
    "                    'visual_field':'down',\n",
    "                    'yhat': oun_down.data.flatten() * 1e6\n",
    "                        })\n",
    "        df = pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8])\n",
    "        df.to_csv(spec_path+f'/uncorrected_{el}_evoked_{sub}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C1 ERP extraction\n",
    "\"\"\"\n",
    "reg_att_up = []\n",
    "odd_att_up = []\n",
    "reg_unatt_up = []\n",
    "odd_unatt_up = []\n",
    "\n",
    "reg_att_down = []\n",
    "odd_att_down = []\n",
    "reg_unatt_down = []\n",
    "odd_unatt_down = []\n",
    "\n",
    "cond_ep_lists = [ep_reg_att,ep_odd_att,ep_reg_unatt,ep_odd_unatt] \n",
    "up_conds = [reg_att_up,odd_att_up,reg_unatt_up,odd_unatt_up]  \n",
    "down_conds = [reg_att_down,odd_att_down,reg_unatt_down,odd_unatt_down]  \n",
    "ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "# List to know which position was used to select the up or down later\n",
    "sub_pos_info = []\n",
    "\n",
    "evokeds_up = dict(zip(ev_names,up_conds))\n",
    "evokeds_down = dict(zip(ev_names,down_conds))\n",
    "\n",
    "for i, sub in enumerate(sub_list):\n",
    "    # which set of positions does this sub have\n",
    "    loc_quad_values = ep_reg_att[f'participant == {sub}'].metadata[\"loc_quad\"]\n",
    "    loc_quad_value = loc_quad_values.unique()[0]\n",
    "    # Dict to stor positional info\n",
    "    sub_info_dict = {}\n",
    "    sub_info_dict['subject'] = sub\n",
    "    # Positions 1 and 3\n",
    "    if loc_quad_value == 0  or loc_quad_value== 2 :\n",
    "        # make dict to store the selected evokeds\n",
    "        ev_position1 = {}\n",
    "        # for every condition\n",
    "        for i, epoch in enumerate(cond_ep_lists):\n",
    "            # transform the epochs into evokeds\n",
    "            ev_position1[ev_names[i]] = epoch['pos1'][f'participant == {sub}'].average()\n",
    "        # same for position 3\n",
    "        ev_position3 = {}\n",
    "        for i, epoch in enumerate(cond_ep_lists):\n",
    "            ev_position3[ev_names[i]] = epoch['pos3'][f'participant == {sub}'].average()\n",
    "        # for every condition in the upper vf\n",
    "        for cond,evoked in ev_position1.items():\n",
    "            evokeds_up[cond].append(evoked)\n",
    "        # same for down\n",
    "        for cond,evoked in ev_position3.items():\n",
    "            evokeds_down[cond].append(evoked)\n",
    "        # Store position information\n",
    "        sub_info_dict['up_pos'] = 1\n",
    "        sub_info_dict['down_pos'] = 3\n",
    "    # same for positions 2 and 4\n",
    "    if loc_quad_value == 1  or loc_quad_value== 3 :\n",
    "        ev_position2 = {}\n",
    "        for i, evoked in enumerate(cond_ep_lists):\n",
    "            ev_position2[ev_names[i]] = evoked['pos2'][f'participant == {sub}'].average()\n",
    "        ev_position4 = {}\n",
    "        for i, evoked in enumerate(cond_ep_lists):\n",
    "            ev_position4[ev_names[i]] = evoked['pos4'][f'participant == {sub}'].average()\n",
    "        # for every condition in the upper vf\n",
    "        for cond,evoked in ev_position2.items():\n",
    "            # append to that conditions list in the up_dict\n",
    "            evokeds_up[cond].append(evoked)\n",
    "        # same for lower vf\n",
    "        for cond,evoked in ev_position4.items():\n",
    "            evokeds_down[cond].append(evoked)\n",
    "        sub_info_dict['up_pos'] = 2\n",
    "        sub_info_dict['down_pos'] = 4\n",
    "    # Add info_dict_to list\n",
    "    sub_pos_info.append(sub_info_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peak properties of localiser data\n",
    "peak_properties = pd.read_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_peak_properties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom dict of lists to list of dicts for plotting individually\n",
    "subj_uplist = [dict(zip(evokeds_up, values)) for values in zip(*evokeds_up.values())]\n",
    "subj_downlist = [dict(zip(evokeds_down, values)) for values in zip(*evokeds_down.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes for export and analysis\n",
    "long_sub_list = [item for item in sub_list for _ in range(4)]\n",
    "c1_long_df = pd.DataFrame(long_sub_list, columns=['subject'])\n",
    "window_halfwidth = 0.01\n",
    "# Condition columns\n",
    "mid_att = []\n",
    "mid_pred = []\n",
    "# General analysis list init \n",
    "genup_amp = []\n",
    "gendown_amp = []\n",
    "general_amp = []\n",
    "# General time window estimate\n",
    "gen_lat = peak_properties['grand_average_peak_latency'][0]\n",
    "gen_tmin = gen_lat - window_halfwidth\n",
    "gen_tmax = gen_lat + window_halfwidth\n",
    "# \n",
    "selectup_gen_evoked = []\n",
    "selectdown_gen_evoked = []\n",
    "\n",
    "# Tailored analysis empty lists\n",
    "tailup_amp = []\n",
    "taildown_amp = []\n",
    "tailored_amp =[]\n",
    "\n",
    "selectup_tail_evoked = []\n",
    "selectdown_tail_evoked = []\n",
    "\n",
    "# Ultra tailored analysis values \n",
    "ultraup_amp = []\n",
    "ultradown_amp = []\n",
    "ultra_amp = []\n",
    "ultraup_tmins = []\n",
    "ultraup_tmaxs = []\n",
    "ultradown_tmins = []\n",
    "ultradown_tmaxs = []\n",
    "\n",
    "selectup_ultra_evoked = []\n",
    "selectdown_ultra_evoked = []\n",
    "\n",
    "conditions_att = ['attended','attended','unattended','unattended',]\n",
    "conditions_pred = ['regular','odd','regular','odd',]\n",
    "conditions = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "\n",
    "# Static C1 window selection\n",
    "for i, sub in enumerate(sub_list):\n",
    "    # Dicts to store the selected evoked\n",
    "    upgen_select_dict = {}\n",
    "    downgen_select_dict = {}\n",
    "    uptail_select_dict = {}\n",
    "    downtail_select_dict = {}\n",
    "    upultra_select_dict = {}\n",
    "    downultra_select_dict = {}\n",
    "    # Select the channel and latency based on localiser info\n",
    "    sub_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    # Tailored info\n",
    "    tailored_channel = sub_row['all_pos_peak_channel'].iloc[0]\n",
    "    tailored_latency = sub_row['all_pos_peak_latency'].iloc[0]\n",
    "    tail_tmin = tailored_latency - window_halfwidth\n",
    "    tail_tmax = tailored_latency + window_halfwidth\n",
    "    # Ultra tailored info\n",
    "    up_pos = sub_pos_info[i]['up_pos']\n",
    "    down_pos = sub_pos_info[i]['down_pos']\n",
    "    ultraup_channel = sub_row[f'pos{up_pos}_peak_channel'].iloc[0]\n",
    "    ultradown_channel = sub_row[f'pos{down_pos}_peak_channel'].iloc[0]\n",
    "    ultraup_latency = sub_row[f'pos{up_pos}_peak_latency'].iloc[0]\n",
    "    ultradown_latency = sub_row[f'pos{up_pos}_peak_latency'].iloc[0]\n",
    "    ultraup_tmin = ultraup_latency - window_halfwidth\n",
    "    [ultraup_tmins.append(ultraup_tmin) for p in range(4)]\n",
    "    ultraup_tmax = ultraup_latency + window_halfwidth\n",
    "    [ultraup_tmaxs.append(ultraup_tmax) for p in range(4)]\n",
    "    ultradown_tmin = ultradown_latency - window_halfwidth\n",
    "    [ultradown_tmins.append(ultradown_tmin) for p in range(4)]\n",
    "    ultradown_tmax = ultradown_latency + window_halfwidth\n",
    "    [ultradown_tmaxs.append(ultradown_tmax) for p in range(4)]\n",
    "\n",
    "    for ind in range(4):\n",
    "        # attended and regular\n",
    "        mid_att.append(conditions_att[ind])\n",
    "        mid_pred.append(conditions_pred[ind])\n",
    "        # General\n",
    "        # Pick channel\n",
    "        gen_selected_up = subj_uplist[i][conditions[ind]].copy().pick(['POz']) \n",
    "        gen_selected_down = subj_downlist[i][conditions[ind]].copy().pick(['POz'])\n",
    "        upgen_select_dict[conditions[ind]] =  gen_selected_up\n",
    "        downgen_select_dict[conditions[ind]] =  gen_selected_down\n",
    "        # Extract data\n",
    "        gen_up_mean_data = gen_selected_up.copy().crop(tmin=gen_tmin,tmax=gen_tmax).data\n",
    "        gen_down_mean_data = gen_selected_down.copy().crop(tmin=gen_tmin,tmax=gen_tmax).data\n",
    "        # Agragate and append\n",
    "        genup_mean = gen_up_mean_data.mean(axis=1) * 1e6\n",
    "        gendown_mean = gen_down_mean_data.mean(axis=1) * 1e6\n",
    "        genup_amp.append(genup_mean[0])\n",
    "        gendown_amp.append(gendown_mean[0])\n",
    "        # RMS\n",
    "        gen_mean = np.mean([np.sqrt(genup_mean**2),np.sqrt(gendown_mean**2)])\n",
    "        general_amp.append(gen_mean)\n",
    "\n",
    "        # Tailored\n",
    "        # Pick channel\n",
    "        tail_selected_up = subj_uplist[i][conditions[ind]].copy().pick([tailored_channel]) \n",
    "        tail_selected_down = subj_downlist[i][conditions[ind]].copy().pick([tailored_channel])\n",
    "        uptail_select_dict[conditions[ind]] =  tail_selected_up\n",
    "        downtail_select_dict[conditions[ind]] =  tail_selected_down\n",
    "        # Extract data\n",
    "        tail_up_mean_data = tail_selected_up.copy().crop(tmin=tail_tmin,tmax=tail_tmax).data\n",
    "        tail_down_mean_data = tail_selected_down.copy().crop(tmin=tail_tmin,tmax=tail_tmax).data\n",
    "        # Agragate and append\n",
    "        tailup_mean = tail_up_mean_data.mean(axis=1) * 1e6\n",
    "        taildown_mean = tail_down_mean_data.mean(axis=1) * 1e6\n",
    "        tailup_amp.append(tailup_mean[0])\n",
    "        taildown_amp.append(taildown_mean[0])\n",
    "        # RMS\n",
    "        tail_mean = np.mean([np.sqrt(tailup_mean**2),np.sqrt(taildown_mean**2)])\n",
    "        tailored_amp.append(tail_mean)\n",
    "\n",
    "        # Ultra tailored\n",
    "        # Pick channel\n",
    "        ultra_selected_up = subj_uplist[i][conditions[ind]].copy().pick([ultraup_channel]) \n",
    "        ultra_selected_down = subj_downlist[i][conditions[ind]].copy().pick([ultradown_channel])\n",
    "        upultra_select_dict[conditions[ind]] =  ultra_selected_up\n",
    "        downultra_select_dict[conditions[ind]] =  ultra_selected_down\n",
    "        # Extract data\n",
    "        ultra_up_mean_data = ultra_selected_up.copy().crop(tmin=ultraup_tmin,tmax=ultraup_tmax).data\n",
    "        ultra_down_mean_data = ultra_selected_down.copy().crop(tmin=ultradown_tmin,tmax=ultradown_tmax).data\n",
    "        # Agragate and append\n",
    "        ultraup_mean = ultra_up_mean_data.mean(axis=1) * 1e6\n",
    "        ultradown_mean = ultra_down_mean_data.mean(axis=1) * 1e6\n",
    "        ultraup_amp.append(ultraup_mean[0])\n",
    "        ultradown_amp.append(ultradown_mean[0])\n",
    "        # RMS\n",
    "        ultra_mean = np.mean([np.sqrt(ultraup_mean**2),np.sqrt(ultradown_mean**2)])\n",
    "        ultra_amp.append(ultra_mean)\n",
    "    \n",
    "    selectup_gen_evoked.append(upgen_select_dict)\n",
    "    selectdown_gen_evoked.append(downgen_select_dict) \n",
    "    selectup_tail_evoked.append(uptail_select_dict)\n",
    "    selectdown_tail_evoked.append(downtail_select_dict) \n",
    "    selectup_ultra_evoked.append(uptail_select_dict)\n",
    "    selectdown_ultra_evoked.append(downtail_select_dict) \n",
    "\n",
    "c1_long_df['attention'] = mid_att\n",
    "c1_long_df['expectation'] = mid_pred\n",
    "c1_long_df['general_amp'] = general_amp\n",
    "c1_long_df['general_up_amp'] = genup_amp\n",
    "c1_long_df['general_down_amp'] = gendown_amp\n",
    "c1_long_df['tailored_amp'] = tailored_amp\n",
    "c1_long_df['tailored_up_amp'] = tailup_amp\n",
    "c1_long_df['tailored_down_amp'] = taildown_amp\n",
    "c1_long_df['ultra_amp'] = ultra_amp\n",
    "c1_long_df['ultra_up_amp'] = ultraup_amp\n",
    "c1_long_df['ultra_down_amp'] = ultradown_amp\n",
    "c1_long_df['ultra_up_tmin'] = ultraup_tmins\n",
    "c1_long_df['ultra_up_tmax'] = ultraup_tmaxs\n",
    "c1_long_df['ultra_down_tmin'] = ultradown_tmins\n",
    "c1_long_df['ultra_down_tmax'] = ultradown_tmaxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct dataframe for subjects 8 and 9 having truncated datasets (lost trials)\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 9) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'odd'), \n",
    "       ['general_up_amp', 'tailored_up_amp', 'ultra_up_amp','general_amp','tailored_amp','ultra_amp']] = np.nan\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 9) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'regular'), \n",
    "       ['general_down_amp', 'tailored_down_amp', 'ultra_down_amp','general_amp','tailored_amp','ultra_amp']] = np.nan\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 8) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'regular'), \n",
    "       ['general_up_amp', 'tailored_up_amp', 'ultra_up_amp','general_amp','tailored_amp','ultra_amp']] = np.nan\n",
    "c1_long_df.loc[(c1_long_df['subject'] == 8) & (c1_long_df['attention'] == 'attended') & (c1_long_df['expectation'] == 'odd'), \n",
    "       ['general_down_amp', 'tailored_down_amp', 'ultra_down_amp','general_amp','tailored_amp','ultra_amp']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual subject plotting\n",
    "scale = [-12,12]\n",
    "att_reg_up_data = [] \n",
    "att_odd_up_data = [] \n",
    "unatt_reg_up_data = [] \n",
    "unatt_odd_up_data = [] \n",
    "\n",
    "att_reg_down_data = [] \n",
    "att_odd_down_data = [] \n",
    "unatt_reg_down_data = [] \n",
    "unatt_odd_down_data = []\n",
    "\n",
    "up_data = [att_reg_up_data,att_odd_up_data,unatt_reg_up_data,unatt_odd_up_data]\n",
    "down_data = [att_reg_down_data,att_odd_down_data,unatt_reg_down_data,unatt_odd_down_data]\n",
    "\n",
    "norm_att_reg_up_data = [] \n",
    "norm_att_odd_up_data = [] \n",
    "norm_unatt_reg_up_data = [] \n",
    "norm_unatt_odd_up_data = [] \n",
    "\n",
    "norm_att_reg_down_data = [] \n",
    "norm_att_odd_down_data = [] \n",
    "norm_unatt_reg_down_data = [] \n",
    "norm_unatt_odd_down_data = [] \n",
    "\n",
    "norm_up_data = [norm_att_reg_up_data,norm_att_odd_up_data,norm_unatt_reg_up_data,norm_unatt_odd_up_data]\n",
    "norm_down_data = [norm_att_reg_down_data,norm_att_odd_down_data,norm_unatt_reg_down_data,norm_unatt_odd_down_data]\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    # init fig\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    # take times from single evoked (should be all the same)\n",
    "    times = evoked_pos1_list[0].times\n",
    "    \n",
    "    the_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    posup = sub_pos_info[i]['up_pos']\n",
    "    posdown = sub_pos_info[i]['down_pos']\n",
    "    channel_up = the_row[f'pos{posup}_peak_channel'].iloc[0]\n",
    "    channel_down = the_row[f'pos{posdown}_peak_channel'].iloc[0]\n",
    "    norm_channel = the_row[f'all_pos_peak_channel'].iloc[0]\n",
    " \n",
    "    ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "    evokeds_up_list = [evokeds_up['reg_att'][i],\n",
    "                       evokeds_up['odd_att'][i],\n",
    "                       evokeds_up['reg_unatt'][i],\n",
    "                       evokeds_up['odd_unatt'][i]]\n",
    "    evokeds_down_list = [evokeds_down['reg_att'][i],\n",
    "                         evokeds_down['odd_att'][i],\n",
    "                         evokeds_down['reg_unatt'][i],\n",
    "                         evokeds_down['odd_unatt'][i]]\n",
    "    for idx in range(4):\n",
    "        up = evokeds_up_list[idx].copy().pick(channel_up).data.squeeze() * 1e6\n",
    "        up_data[idx].append(up)\n",
    "        norm_up_data[idx].append(evokeds_up_list[idx].copy().pick(norm_channel).data.squeeze() * 1e6)\n",
    "        sns.lineplot(x=times, y=up, ax = axes[0], label = ev_names[idx])\n",
    "        down = evokeds_down_list[idx].copy().pick(channel_down).data.squeeze() * 1e6\n",
    "        down_data[idx].append(down)\n",
    "        norm_down_data[idx].append(evokeds_down_list[idx].copy().pick(norm_channel).data.squeeze() * 1e6)\n",
    "        sns.lineplot(x=times, y=down, ax = axes[1],label = ev_names[idx])\n",
    "\n",
    "    # mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "    the_row = c1_long_df[c1_long_df['subject'] == sub_pos_info[i]['subject']]\n",
    "    up_tmin= the_row['ultra_up_tmin'].iloc[0]\n",
    "    up_tmax= the_row['ultra_up_tmax'].iloc[0]\n",
    "    down_tmin= the_row['ultra_down_tmin'].iloc[0]\n",
    "    down_tmax= the_row['ultra_down_tmax'].iloc[0]\n",
    "\n",
    "    axes[0].axhline(y=0, lw=1, c='black' )\n",
    "    axes[1].axhline(y=0, lw=1, c='black' )\n",
    "    axes[0].axvline(x=up_tmin,ls='--',lw=1, c='black' )\n",
    "    axes[0].axvline(x=up_tmax,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=down_tmin,ls='--',lw=1, c='black' )\n",
    "    axes[1].axvline(x=down_tmax,ls='--',lw=1, c='black' )\n",
    "    axes[0].set_title('Up')\n",
    "    axes[1].set_title('Down')\n",
    "    axes[0].set_ylabel('µV')\n",
    "    axes[0].set_xlabel('time (s)')\n",
    "    axes[1].set_xlabel('time (s)')\n",
    "\n",
    "    plt.title(f\"subject {sub}\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something convoluted to get the POz data in an array\n",
    "gen_att_reg_up_data = [] \n",
    "gen_att_odd_up_data = [] \n",
    "gen_unatt_reg_up_data = [] \n",
    "gen_unatt_odd_up_data = [] \n",
    "\n",
    "gen_att_reg_down_data = [] \n",
    "gen_att_odd_down_data = [] \n",
    "gen_unatt_reg_down_data = [] \n",
    "gen_unatt_odd_down_data = [] \n",
    "\n",
    "gen_up_data = [gen_att_reg_up_data,gen_att_odd_up_data,gen_unatt_reg_up_data,gen_unatt_odd_up_data]\n",
    "gen_down_data = [gen_att_reg_down_data,gen_att_odd_down_data,gen_unatt_reg_down_data,gen_unatt_odd_down_data]\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    # take times from single evoked (should be all the same)\n",
    "    times = evoked_pos1_list[0].times\n",
    "    \n",
    "    the_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "\n",
    "    ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "    evokeds_up_list = [evokeds_up['reg_att'][i],\n",
    "                       evokeds_up['odd_att'][i],\n",
    "                       evokeds_up['reg_unatt'][i],\n",
    "                       evokeds_up['odd_unatt'][i]]\n",
    "    evokeds_down_list = [evokeds_down['reg_att'][i],\n",
    "                         evokeds_down['odd_att'][i],\n",
    "                         evokeds_down['reg_unatt'][i],\n",
    "                         evokeds_down['odd_unatt'][i]]\n",
    "    for idx in range(4):\n",
    "        gen_up_data[idx].append(evokeds_up_list[idx].copy().pick('POz').data.squeeze() * 1e6)\n",
    "        gen_down_data[idx].append(evokeds_down_list[idx].copy().pick('POz').data.squeeze() * 1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [-12,12]\n",
    "for i,sub in enumerate(sub_list):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6), sharey=True)\n",
    "    the_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    pos = sub_pos_info[i]['down_pos']\n",
    "    channel = the_row[f'pos{pos}_peak_channel'].iloc[0]\n",
    "     \n",
    "    ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "    evokeds_list = [evokeds_down['reg_att'][i],\n",
    "                    evokeds_down['odd_att'][i],\n",
    "                    evokeds_down['reg_unatt'][i],\n",
    "                    evokeds_down['odd_unatt'][i]]\n",
    "    \n",
    "    # conds = ('seq1','seq2','seq3','seq4')\n",
    "    norm = dict(zip(ev_names, evokeds_list))\n",
    "\n",
    "    mne.viz.plot_compare_evokeds(norm, picks='POz', axes=axes[0], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    mne.viz.plot_compare_evokeds(norm, picks=channel, axes=axes[1], vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of extracted C1 erp\n",
    "c1_up_df = c1_long_df[['subject','attention','expectation','general_up_amp','tailored_up_amp','ultra_up_amp']]\n",
    "c1_down_df = c1_long_df[['subject','attention','expectation','general_down_amp','tailored_down_amp','ultra_down_amp']]\n",
    "c1_up_df['visual_field'] = 'up'\n",
    "c1_down_df['visual_field'] = 'down'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_up_df = c1_up_df.rename(columns={'general_up_amp':'general_amp', 'tailored_up_amp':'tailored_amp', 'ultra_up_amp': 'ultra_amp'})\n",
    "c1_down_df = c1_down_df.rename(columns={'general_down_amp':'general_amp', 'tailored_down_amp':'tailored_amp', 'ultra_down_amp': 'ultra_amp'})\n",
    "c1_very_long_df = pd.concat([c1_up_df,c1_down_df],axis = 0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files with latencies based on localiser and 20 ms window\n",
    "c1_very_long_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_long_df.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All remaining code below is data exploration and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evoked_ga_up = [mne.grand_average(evokeds_up['reg_att']),\n",
    "                mne.grand_average(evokeds_up['odd_att']),\n",
    "                mne.grand_average(evokeds_up['reg_unatt']),\n",
    "                mne.grand_average(evokeds_up['odd_unatt'])]\n",
    "\n",
    "evoked_ga_down = [mne.grand_average(evokeds_down['reg_att']),\n",
    "                  mne.grand_average(evokeds_down['odd_att']),\n",
    "                  mne.grand_average(evokeds_down['reg_unatt']),\n",
    "                  mne.grand_average(evokeds_down['odd_unatt'])]\n",
    "\n",
    "evoked_ga = []\n",
    "\n",
    "for i,(up,down) in enumerate(zip(evoked_ga_up,evoked_ga_down)):\n",
    "    rms_data = np.mean([np.sqrt(up.data**2),np.sqrt(down.data**2)],axis=0)\n",
    "    rms_evoked = mne.EvokedArray(rms_data,info=up.info, tmin=up.times[0])\n",
    "    evoked_ga.append(rms_evoked)\n",
    "\n",
    "conditions = ('attended regular','attended odd','unattended regular','unattended odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Get up vs down topograhpy\n",
    "\"\"\"\n",
    "up_ga = mne.grand_average(evoked_ga_up)\n",
    "down_ga = mne.grand_average(evoked_ga_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = np.linspace(0.089, 0.1, 1)\n",
    "lim = (-3,3)\n",
    "# up_ga.plot_topomap(ch_type=\"eeg\", times= tim, colorbar=True, vlim = lim)\n",
    "down_ga.plot_topomap(ch_type=\"eeg\", times= tim, colorbar=True, vlim = lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_up_array = np.array(gen_up_data)\n",
    "gen_down_array = np.array(gen_down_data)\n",
    "up_array = np.array(up_data)\n",
    "down_array = np.array(down_data)\n",
    "norm_up_array = np.array(norm_up_data)\n",
    "norm_down_array = np.array(norm_down_data)\n",
    "# drop bad spans\n",
    "gen_up_array[0,6,:] = np.nan\n",
    "gen_down_array[1,6,:] = np.nan\n",
    "gen_up_array[1,7,:] = np.nan\n",
    "gen_down_array[0,7,:] = np.nan\n",
    "up_array[0,6,:] = np.nan\n",
    "down_array[1,6,:] = np.nan\n",
    "up_array[1,7,:] = np.nan\n",
    "down_array[0,7,:] = np.nan\n",
    "norm_up_array[0,6,:] = np.nan\n",
    "norm_down_array[1,6,:] = np.nan\n",
    "norm_up_array[1,7,:] = np.nan\n",
    "norm_down_array[0,7,:] = np.nan\n",
    "\n",
    "gen_up_total = np.nanmean(gen_up_array,axis=1)\n",
    "gen_down_total = np.nanmean(gen_down_array,axis=1)\n",
    "up_total = np.nanmean(up_array,axis=1)\n",
    "down_total = np.nanmean(down_array,axis=1)\n",
    "norm_up_total = np.nanmean(norm_up_array,axis=1)\n",
    "norm_down_total = np.nanmean(norm_down_array,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create plottign df\n",
    "\"\"\"\n",
    "# Flatten the array (i.e., stack all the data into one column)\n",
    "flat_up = gen_up_array.flatten() # norm_up_array.flatten()\n",
    "flat_down = gen_down_array.flatten() # norm_down_array.flatten()\n",
    "flat = np.concatenate((flat_up,flat_down))\n",
    "times = evoked_pos1_list[0].times\n",
    "# Create arrays for position, subject, and timepoint\n",
    "rep_conds = np.repeat(conditions, 23 * 155)  # Repeat each position 23 * 155 times\n",
    "subjects = np.tile(np.repeat(sub_list, 155), 4)  # Repeat each subject 155 times for each position\n",
    "timepoints = np.tile(times, 4 * 23)  \n",
    "\n",
    "# Create the DataFrame\n",
    "up_plot_df = pd.DataFrame({\n",
    "    'subject': subjects,\n",
    "    'times': timepoints,\n",
    "    'condition': rep_conds,\n",
    "    'amp': flat_up\n",
    "})\n",
    "# Create the DataFrame\n",
    "down_plot_df = pd.DataFrame({\n",
    "    'subject': subjects,\n",
    "    'times': timepoints,\n",
    "    'condition': rep_conds,\n",
    "    'amp': flat_down\n",
    "})\n",
    "\n",
    "vf_field = np.repeat(['up', 'down'], 4 *23 * 155) \n",
    "# Create the DataFrame\n",
    "plot_df = pd.DataFrame({\n",
    "    'subject': np.tile(subjects,2),\n",
    "    'times': np.tile(timepoints,2),\n",
    "    'condition': np.tile(rep_conds,2),\n",
    "    'visual_field':vf_field,\n",
    "    'amp': flat\n",
    "})\n",
    "\n",
    "plot_df['rms_amp'] = np.sqrt(plot_df['amp']**2)\n",
    "# Split the column based on an underscore (_)\n",
    "plot_df[['attention', 'expectation']] = plot_df['condition'].str.split(' ', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files with latencies based on localiser and 20 ms window\n",
    "plot_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\plotting_df\\standard_c1_plot.csv',index= False)\n",
    "# plot_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\plotting_df\\indivualised_c1_plot.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init fig\n",
    "fig, axes = plt.subplots(2,3, figsize=(12, 6), sharey=True)\n",
    "times = evoked_pos1_list[0].times\n",
    "\n",
    "for idx in range(4):\n",
    "    up_ga = evoked_ga_up[idx].get_data(['POz']).squeeze() * 1e6\n",
    "    sns.lineplot(x=times, y=up_ga, ax = axes[0,0], label = ev_names[idx])\n",
    "    norm_up = norm_up_total[idx,:]\n",
    "    sns.lineplot(x=times, y=norm_up, ax = axes[0,1], label = ev_names[idx])\n",
    "    up = up_total[idx,:]\n",
    "    sns.lineplot(x=times, y=up, ax = axes[0,2], label = ev_names[idx])\n",
    "\n",
    "    down_ga = evoked_ga_down[idx].get_data(['POz']).squeeze() * 1e6\n",
    "    sns.lineplot(x=times, y=down_ga, ax = axes[1,0],label = ev_names[idx])\n",
    "    norm_down = norm_down_total[idx,:]\n",
    "    sns.lineplot(x=times, y=norm_down, ax = axes[1,1],label = ev_names[idx])\n",
    "    down = down_total[idx,:]\n",
    "    sns.lineplot(x=times, y=down, ax = axes[1,2],label = ev_names[idx])\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.axhline(y=0, lw=1, c='black' )\n",
    "    ax.axhline(y=-4, ls ='--',lw=1, c='black' )\n",
    "    ax.axhline(y=4, ls ='--',lw=1, c='black' )\n",
    "    ax.axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    ax.axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "sns.set_context(\"paper\")\n",
    "axes[0,0].set_title('Up POz')\n",
    "axes[0,1].set_title('Up variable')\n",
    "axes[0,2].set_title('Up very variable')\n",
    "axes[1,0].set_title('Down POz')\n",
    "axes[1,1].set_title('Down variable')\n",
    "axes[1,2].set_title('Down very variable')\n",
    "axes[0,0].set_ylabel('µV')\n",
    "axes[1,0].set_ylabel('µV')\n",
    "axes[1,0].set_xlabel('time (s)')\n",
    "axes[1,1].set_xlabel('time (s)')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make rms plots for tailored evokeds\n",
    "rms_total = np.mean([np.sqrt(norm_up_total**2),np.sqrt(norm_down_total**2)],axis=0)\n",
    "rmsrms = np.sqrt(rms_total**2)\n",
    "fig, axes = plt.subplots()\n",
    "times = evoked_pos1_list[0].times\n",
    "handles = ['Regular Attended','Odd Attended','Regular Unattented','Odd Unattended']\n",
    "\n",
    "for idx in range(4):\n",
    "    gwa = rms_total[idx,:]\n",
    "    sns.lineplot(x=times, y=gwa, label = handles[idx])\n",
    "\n",
    "axes.axhline(y=0, lw=1, c='black' )\n",
    "axes.axhline(y=-4, ls ='--',lw=1, c='black' )\n",
    "axes.axhline(y=4, ls ='--',lw=1, c='black' )\n",
    "axes.axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "axes.axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "\n",
    "axes.set_title('Ultra RMS')\n",
    "axes.set_ylabel('µV')\n",
    "axes.set_xlabel('time (s)')\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(16, 10), sharey=True, facecolor ='#C1E5F5')\n",
    "\n",
    "times = evoked_pos1_list[0].times\n",
    "handles = ['Regular Attended','Odd Attended','Regular Unattented','Odd Unattended']\n",
    "line_colours = {'attended reg':my_palette[1],'attended odd':my_palette[1],'unattended reg':my_palette[2],'unattended odd':my_palette[2]}\n",
    "\n",
    "att_up_vf = mlines.Line2D([], [], color=my_palette[1], linestyle='-', label='Attended Regular',linewidth=6)\n",
    "unatt_up_vf = mlines.Line2D([], [], color=my_palette[1], linestyle='dotted', label='Attended Odd',linewidth=6)\n",
    "att_down_vf = mlines.Line2D([], [], color= my_palette[2], linestyle='-', label='Unattented Regular',linewidth=6)\n",
    "unatt_down_vf = mlines.Line2D([], [], color=my_palette[2], linestyle='dotted', label='Unattended Odd',linewidth=6)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "sns.lineplot(data=plot_df, x='times', y='rms_amp',hue='condition',\n",
    "             palette = line_colours,ax=axes, linewidth=6,\n",
    "             errorbar = 'se')\n",
    "# Might need to loop through the list if there are multiple lines on the plot\n",
    "axes.lines[1].set_linestyle(\"dotted\")\n",
    "axes.lines[3].set_linestyle(\"dotted\")\n",
    "\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "plt.rcParams['font.weight'] = 'semibold'\n",
    "legend = plt.legend(handles=[att_up_vf, unatt_up_vf,att_down_vf,unatt_down_vf,],\n",
    "           loc='upper left', bbox_to_anchor=(-.01, 1),prop={'size':22},facecolor= 'white')\n",
    "\n",
    "plt.gca().set_facecolor('#C1E5F5')\n",
    "sns.despine(offset=10, trim=True);\n",
    "for i in range(1):\n",
    "    axes.axhline(y=0, lw=1, c='black' )\n",
    "    axes.axhline(y=4, ls ='--',lw=1, c='black' )\n",
    "    axes.axvline(x=.05,ls='--',lw=1, c='black' )\n",
    "    axes.axvline(x=.1,ls='--',lw=1, c='black' )\n",
    "    axes.set_ylabel('µV (absolute value)', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "    axes.set_xlabel('time (s)',fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "\n",
    "sns.axes_style(\"ticks\")\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(16, 6), sharey=True, facecolor ='#C1E5F5')\n",
    "\n",
    "c1_very_long_df['rms_tailored'] = np.sqrt(c1_very_long_df['tailored_amp']**2)\n",
    "\n",
    "att_label = mlines.Line2D([], [], color=my_palette[4], linestyle='-', label='Attended',linewidth=6)\n",
    "unatt_label = mlines.Line2D([], [], color=my_palette[8], linestyle='-', label='Unattended',linewidth=6)\n",
    "reg_label = mlines.Line2D([], [], color= my_palette[9], linestyle='-', label='Regular',linewidth=6)\n",
    "odd_label = mlines.Line2D([], [], color=my_palette[6], linestyle='-', label='Odd',linewidth=6)\n",
    "\n",
    "sns.barplot(c1_very_long_df, y= 'rms_tailored', x= 'visual_field', hue= 'attention', edgecolor= 'none', palette= [my_palette[4],my_palette[8]],\n",
    "            errorbar='se', errwidth= 5,\n",
    "            capsize=.1,width=.4, linewidth=0.7, ax=axes[0])\n",
    "\n",
    "sns.barplot(c1_very_long_df , y= 'rms_tailored', x= 'visual_field', hue= 'expectation', edgecolor= 'none', palette= [my_palette[9],my_palette[6]],\n",
    "            errorbar='se', errwidth= 5,\n",
    "            capsize=.1,width=.4, linewidth=0.7, ax=axes[1])\n",
    "\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "plt.rcParams['font.weight'] = 'semibold'\n",
    "sns.set_context(\"poster\")\n",
    "axes[0].set_ylim(0,5)\n",
    "axes[0].set_facecolor('#C1E5F5')\n",
    "axes[1].set_facecolor('#C1E5F5')\n",
    "axes[0].legend(handles=[att_label,unatt_label],\n",
    "           loc='upper left', bbox_to_anchor=(.65, 1),prop={'size':22},facecolor= 'white')\n",
    "axes[1].legend(handles=[reg_label,odd_label],\n",
    "           loc='upper left', bbox_to_anchor=(.65, 1),prop={'size':22},facecolor= 'white')\n",
    "\n",
    "axes[0].set_ylabel('µV (absolute value)', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "axes[0].set_xlabel('Visual Field', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "axes[1].set_xlabel('Visual Field', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':28})\n",
    "\n",
    "sns.despine(offset=10, trim=True);\n",
    "sns.axes_style(\"ticks\")\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, figsize=(16, 10), sharey=True, facecolor ='#C1E5F5')\n",
    "\n",
    "c1_very_long_df['rms_tailored'] = np.sqrt(c1_very_long_df['tailored_amp']**2)\n",
    "\n",
    "att_label = mlines.Line2D([], [], color=my_palette[4], linestyle='-', label='Attended',linewidth=6)\n",
    "unatt_label = mlines.Line2D([], [], color=my_palette[8], linestyle='-', label='Unattended',linewidth=6)\n",
    "reg_label = mlines.Line2D([], [], color= my_palette[9], linestyle='-', label='Regular',linewidth=6)\n",
    "odd_label = mlines.Line2D([], [], color=my_palette[6], linestyle='-', label='Odd',linewidth=6)\n",
    "\n",
    "bar_plot= sns.barplot(p3_df, y= 'mean_amp', x= 'expectation', hue= 'attention', palette= [my_palette[4],my_palette[8]],\n",
    "            errorbar='se', errwidth= 7,\n",
    "            capsize=.1,width=.4, linewidth=0.7)\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "plt.rcParams['font.weight'] = 'semibold'\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "axes.set_facecolor('#C1E5F5')\n",
    "\n",
    "axes.legend(handles=[att_label,unatt_label],\n",
    "           loc='upper left', bbox_to_anchor=(.9, 1),prop={'size':36},facecolor= 'white')\n",
    "\n",
    "axes.set_ylim(0,3)\n",
    "axes.set_ylabel('µV', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':48})\n",
    "axes.set_xlabel('Expectation', fontdict={'family': 'Segoe UI', 'weight' : 'semibold','size':48})\n",
    "\n",
    "bar_plot.spines['left'].set_linewidth(6)   # Set thickness for left spine\n",
    "bar_plot.spines['bottom'].set_linewidth(6)\n",
    "plt.tick_params(axis='both', width=6, length=12,labelsize = 36)\n",
    "\n",
    "sns.despine(offset=10, trim=True);\n",
    "sns.axes_style(\"ticks\")\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P1 and N1\n",
    "\"\"\"\n",
    "p1_els = ['PO4','PO8','O2','PO3','PO7','O1']\n",
    "ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "\n",
    "# make a df with the data we want\n",
    "time = np.tile(evokeds_up['reg_att'][0].times,8)\n",
    "el_li = np.empty()\n",
    "for ec in ev_names:\n",
    "    ddd2= evokeds_up['reg_att'][0].copy().pick(p1_els).T\n",
    "    el_li.extend()\n",
    "\n",
    "\n",
    "\n",
    "# position_up = np.full(len(time)/2,sub_pos_info[i]['up_pos'])\n",
    "# position_down = np.full(len(time)/2,sub_pos_info[i]['down_pos'])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
